{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4",
            "name": "Track B \u2013 Unsloth SFT"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# \ud83e\udde1 Track B \u2013 Unsloth Instruction Tuning (Memory Optimized)\n\nFine-tune `Qwen/Qwen2.5-Coder-1.5B-Instruct` (or 7B!) on the synthetic Python coding dataset using **Unsloth** for memory efficiency and speed on a free Colab T4 GPU.\n\n### Why Unsloth?\n- **Zero OOM**: Uses 4-bit quantization with minimal accuracy loss.\n- **Faster Training**: Up to 2x speedup compared to standard HF+PEFT.\n- **Larger Batch Sizes**: Enables longer context lengths and larger batch sizes on T4.\n\n---\n\n## \ud83d\udce6 Dataset: [`archit11/track_b_sft`](https://huggingface.co/datasets/archit11/track_b_sft)\n\n| Field | Detail |\n|-------|--------|\n| **Source** | [verl](https://github.com/volcengine/verl) Python library (AST-extracted functions) |\n| **Train** | 514 examples |\n| **Test** | 46 examples |\n| **Pair format** | `instruction` \u2192 `response` (Python code) |\n| **License** | Apache 2.0 |\n\n### Data Card Summary\n\n| Category | Description | Count (Train) |\n|----------|-------------|---------------|\n| `docstring` | Write Google-style docstrings | ~100 |\n| `explain` | Explain function logic | ~100 |\n| `bugfix` | Fix injected bugs (off-by-one, logic errors) | ~100 |\n| `complete` | Complete function bodies | ~100 |\n| `unit_test` | Write pytest unit tests | ~100 |\n\n---\n\n## \ud83e\udd16 Model: `Qwen/Qwen2.5-Coder-1.5B-Instruct` (4-bit)\n\n| Field | Detail |\n|-------|--------|\n| **Base** | `Qwen/Qwen2.5-Coder-1.5B-Instruct` |\n| **Method** | LoRA + 4-bit Quantization (QLoRA) |\n| **Quantization** | 4-bit NF4 |\n| **Gradient Checkpointing** | Enabled (Unsloth optimized) |\n| **Target Modules** | All linear layers |\n| **LoRA Rank** | 16 |\n\n> \u26a1 **Make sure Runtime \u2192 Change runtime type \u2192 T4 GPU is selected before running.**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 \u2013 Install Unsloth (Robust Setup)\n",
                "%%capture\n",
                "import os, re\n",
                "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
                "    !pip install unsloth\n",
                "else:\n",
                "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
                "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
                "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
                "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
                "!pip install transformers==4.56.2\n",
                "!pip install --no-deps trl==0.22.2\n",
                "\n",
                "print(\"\u2713 Unsloth installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 \u2013 Load Model & Tokenizer\n",
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",  # Can swap to \"Qwen/Qwen2.5-Coder-7B-Instruct\" if you feel adventurous!\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")\n",
                "\n",
                "# Add LoRA adapters\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3 \u2013 Format Dataset\n",
                "from datasets import load_dataset\n",
                "\n",
                "dataset = load_dataset(\"archit11/track_b_sft\", split = \"train\")\n",
                "\n",
                "# Apply standard ChatML template using tokenizer.apply_chat_template\n",
                "from unsloth.chat_templates import get_chat_template\n",
                "\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template = \"chatml\", # Qwen uses ChatML-like format\n",
                "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"} # Standard ShareGPT mapping if needed, but we essentially build raw text\n",
                ")\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    convos = []\n",
                "    for i in range(len(examples[\"instruction\"])):\n",
                "        # Construct conversation\n",
                "        c = [\n",
                "            {\"role\": \"user\", \"content\": examples[\"instruction\"][i]},\n",
                "            {\"role\": \"assistant\", \"content\": examples[\"response\"][i]},\n",
                "        ]\n",
                "        # Apply template\n",
                "        text = tokenizer.apply_chat_template(c, tokenize=False, add_generation_prompt=False)\n",
                "        convos.append(text)\n",
                "    return { \"text\" : convos }\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
                "print(dataset[\"text\"][0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4 \u2013 Train\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from unsloth import is_bfloat16_supported\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False, # Can be True for speed, but standard SFT usually fine without packing for short instrs\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        num_train_epochs = 1,\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not is_bfloat16_supported(),\n",
                "        bf16 = is_bfloat16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "        output_dir = \"outputs\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5 \u2013 Inference\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "msg = [\n",
                "    {\"role\": \"user\", \"content\": \"Write a python function to check if a number is prime.\"}\n",
                "]\n",
                "inputs = tokenizer.apply_chat_template(msg, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "outputs = model.generate(inputs, max_new_tokens = 256, use_cache = True)\n",
                "print(tokenizer.decode(outputs[0]))"
            ]
        }
    ]
}