{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4",
            "name": "Track C \u2013 Embedding Fine-tuning Demo"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# \ud83e\udde0 Track C \u2013 Embedding Fine-tuning Demo\n\nFine-tune `Qwen/Qwen3-Embedding-0.6B` on a synthetic text\u2192code retrieval dataset and measure improvement on MRR@10, nDCG@10, and Recall@10.\n\n---\n\n## \ud83d\udce6 Dataset: [`archit11/code-embedding-dataset`](https://huggingface.co/datasets/archit11/code-embedding-dataset)\n\n| Field | Detail |\n|-------|--------|\n| **Source** | [juspay/hyperswitch](https://github.com/juspay/hyperswitch) Rust codebase |\n| **Train** | 1,736 (query, code) pairs |\n| **Test** | 24 held-out retrieval examples |\n| **Pair format** | `sentence_0` = natural-language query, `sentence_1` = code snippet |\n| **License** | MIT |\n\n### Data Card Summary\n\n| Field | Detail |\n|-------|--------|\n| **Generation model** | `qwen/qwen3.5-397b-a17b` via OpenRouter |\n| **Method** | LLM-generated NL descriptions + query variants per Rust file |\n| **Code format** | `// PATH: ...\\n// MODULE: ...\\n// SYMBOL: ...\\n<code>` |\n| **Query types** | Functional (\"how to X\"), structural (\"Y struct definition\"), conceptual (\"what does Z do\") |\n\n---\n\n## \ud83e\udd16 Model: [`archit11/assesment_qwen3_embedding_06b_e3`](https://huggingface.co/archit11/assesment_qwen3_embedding_06b_e3)\n\n| Field | Detail |\n|-------|--------|\n| **Base** | `Qwen/Qwen3-Embedding-0.6B` |\n| **Method** | Full fine-tune via `sentence-transformers` |\n| **Loss** | `MultipleNegativesRankingLoss` (scale=20, cos_sim) |\n| **Epochs** | ~2.3 (500 steps) |\n| **Final train loss** | 0.0380 |\n| **Pooling** | Last-token, 1024-dim, cosine similarity |\n| **Hardware** | T4 GPU |\n\n---\n\n## \ud83d\udcca Results (Reproduced Below)\n\n| Metric | Baseline (`Qwen3-Embedding-0.6B`) | Fine-tuned | \u0394 |\n|--------|----------------------------------|------------|---|\n| **MRR@10** | \u2014 | \u2014 | \u2014 |\n| **nDCG@10** | \u2014 | \u2014 | \u2014 |\n| **Recall@10** | \u2014 | \u2014 | \u2014 |\n\n> \u26a1 **Make sure Runtime \u2192 Change runtime type \u2192 T4 GPU is selected before running.**\n\n> \ud83d\udccc Results table will be filled in automatically by Cell 7."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 \u2013 Install dependencies\n",
                "!pip install -q sentence-transformers==3.4.1 datasets huggingface_hub\n",
                "# sentence-transformers 3.x ships MultipleNegativesRankingLoss + SentenceTransformerTrainer\n",
                "print(\"\u2713 Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 \u2013 Imports & config\n",
                "import math, time, json\n",
                "import numpy as np\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "from sentence_transformers import SentenceTransformer, losses\n",
                "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
                "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
                "\n",
                "BASE_MODEL    = \"Qwen/Qwen3-Embedding-0.6B\"\n",
                "FINETUNED_HF  = \"archit11/assesment_qwen3_embedding_06b_e3\"\n",
                "DATASET_REPO  = \"archit11/code-embedding-dataset\"\n",
                "OUTPUT_DIR    = \"/content/track_c_embedding\"\n",
                "EPOCHS        = 2\n",
                "BATCH_SIZE    = 8       # T4-safe (0.6B model)\n",
                "LR            = 2e-5\n",
                "WARMUP_RATIO  = 0.1\n",
                "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"\u2713 Device : {DEVICE}\")\n",
                "print(f\"\u2713 Base   : {BASE_MODEL}\")\n",
                "print(f\"\u2713 Dataset: {DATASET_REPO}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3 \u2013 Load dataset from Hugging Face\n",
                "print(f\"Loading {DATASET_REPO} ...\")\n",
                "ds = load_dataset(DATASET_REPO)\n",
                "\n",
                "# Training split: sentence_0 = query, sentence_1 = code\n",
                "train_ds = ds[\"train\"]\n",
                "print(f\"\u2713 Train : {len(train_ds)} pairs\")\n",
                "print(f\"  Columns: {train_ds.column_names}\")\n",
                "\n",
                "# Build a retrieval test set from the 'test' split (or fall back to last 24 of train)\n",
                "if \"test\" in ds:\n",
                "    test_ds = ds[\"test\"]\n",
                "    print(f\"\u2713 Test  : {len(test_ds)} examples\")\n",
                "else:\n",
                "    # Use last 24 train examples as held-out test\n",
                "    test_ds = train_ds.select(range(len(train_ds) - 24, len(train_ds)))\n",
                "    train_ds = train_ds.select(range(len(train_ds) - 24))\n",
                "    print(f\"\u2713 Test  : {len(test_ds)} examples (held-out from train)\")\n",
                "\n",
                "# Preview\n",
                "ex = train_ds[0]\n",
                "print(f\"\\nSample pair:\")\n",
                "print(f\"  Query : {ex['sentence_0'][:120]}\")\n",
                "print(f\"  Code  : {str(ex['sentence_1'])[:120]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4 \u2013 Evaluation helpers (MRR@k, nDCG@k, Recall@k)\n",
                "\n",
                "def build_retrieval_corpus(test_data):\n",
                "    \"\"\"\n",
                "    Build a retrieval corpus from the test set.\n",
                "    Each test example contributes:\n",
                "      - 1 positive code document (the paired code)\n",
                "      - All other codes in the test set act as distractors\n",
                "    Returns: queries, corpus, relevance dict\n",
                "    \"\"\"\n",
                "    queries  = {}   # qid -> query text\n",
                "    corpus   = {}   # did -> code text\n",
                "    relevant = {}   # qid -> set of relevant dids\n",
                "\n",
                "    for i, ex in enumerate(test_data):\n",
                "        qid = f\"q{i}\"\n",
                "        did = f\"d{i}\"\n",
                "        # Use 'queries' field if available (list of 4 queries), else sentence_0\n",
                "        if \"queries\" in ex and ex[\"queries\"]:\n",
                "            q = ex[\"queries\"][0] if isinstance(ex[\"queries\"], list) else ex[\"queries\"]\n",
                "        else:\n",
                "            q = ex[\"sentence_0\"]\n",
                "        # Use 'anchor' (code) if available, else sentence_1\n",
                "        code = ex.get(\"anchor\", ex.get(\"sentence_1\", \"\"))\n",
                "        queries[qid]  = q\n",
                "        corpus[did]   = code\n",
                "        relevant[qid] = {did}\n",
                "\n",
                "    return queries, corpus, relevant\n",
                "\n",
                "\n",
                "def compute_retrieval_metrics(model, queries, corpus, relevant, k=10, batch_size=32):\n",
                "    \"\"\"\n",
                "    Encode queries and corpus, rank by cosine similarity,\n",
                "    compute MRR@k, nDCG@k, Recall@k.\n",
                "    \"\"\"\n",
                "    qids   = list(queries.keys())\n",
                "    dids   = list(corpus.keys())\n",
                "    qtexts = [queries[q] for q in qids]\n",
                "    dtexts = [corpus[d]  for d in dids]\n",
                "\n",
                "    # Use encode_query / encode_document if available (Qwen3 asymmetric encoding)\n",
                "    if hasattr(model, \"encode_query\"):\n",
                "        q_embs = model.encode_query(qtexts,  batch_size=batch_size, show_progress_bar=False)\n",
                "        d_embs = model.encode_document(dtexts, batch_size=batch_size, show_progress_bar=False)\n",
                "    else:\n",
                "        q_embs = model.encode(qtexts,  batch_size=batch_size, show_progress_bar=False,\n",
                "                              normalize_embeddings=True)\n",
                "        d_embs = model.encode(dtexts,  batch_size=batch_size, show_progress_bar=False,\n",
                "                              normalize_embeddings=True)\n",
                "\n",
                "    # Cosine similarity matrix [n_queries x n_docs]\n",
                "    sims = np.dot(q_embs, d_embs.T)   # already L2-normalised\n",
                "\n",
                "    mrr_scores, ndcg_scores, recall_scores = [], [], []\n",
                "\n",
                "    for qi, qid in enumerate(qids):\n",
                "        rel_dids = relevant[qid]\n",
                "        ranked   = np.argsort(-sims[qi])[:k]   # top-k doc indices\n",
                "\n",
                "        # MRR@k\n",
                "        mrr = 0.0\n",
                "        for rank, di in enumerate(ranked, 1):\n",
                "            if dids[di] in rel_dids:\n",
                "                mrr = 1.0 / rank\n",
                "                break\n",
                "        mrr_scores.append(mrr)\n",
                "\n",
                "        # nDCG@k\n",
                "        dcg  = sum(1.0 / math.log2(rank + 1)\n",
                "                   for rank, di in enumerate(ranked, 1)\n",
                "                   if dids[di] in rel_dids)\n",
                "        idcg = sum(1.0 / math.log2(rank + 1)\n",
                "                   for rank in range(1, min(len(rel_dids), k) + 1))\n",
                "        ndcg_scores.append(dcg / idcg if idcg > 0 else 0.0)\n",
                "\n",
                "        # Recall@k\n",
                "        hits = sum(1 for di in ranked if dids[di] in rel_dids)\n",
                "        recall_scores.append(hits / len(rel_dids))\n",
                "\n",
                "    return {\n",
                "        \"MRR@10\":    float(np.mean(mrr_scores)),\n",
                "        \"nDCG@10\":   float(np.mean(ndcg_scores)),\n",
                "        \"Recall@10\": float(np.mean(recall_scores)),\n",
                "        \"n_queries\":  len(qids),\n",
                "        \"n_docs\":     len(dids),\n",
                "    }\n",
                "\n",
                "\n",
                "def evaluate_model(model, test_data, tag, k=10):\n",
                "    print(f\"\\n{'='*55}\")\n",
                "    print(f\"  Evaluating: {tag}\")\n",
                "    print(f\"{'='*55}\")\n",
                "    queries, corpus, relevant = build_retrieval_corpus(test_data)\n",
                "    t0 = time.time()\n",
                "    metrics = compute_retrieval_metrics(model, queries, corpus, relevant, k=k)\n",
                "    elapsed = time.time() - t0\n",
                "    print(f\"  MRR@{k}    : {metrics['MRR@10']:.4f}\")\n",
                "    print(f\"  nDCG@{k}   : {metrics['nDCG@10']:.4f}\")\n",
                "    print(f\"  Recall@{k} : {metrics['Recall@10']:.4f}\")\n",
                "    print(f\"  Corpus    : {metrics['n_queries']} queries x {metrics['n_docs']} docs\")\n",
                "    print(f\"  Wall time : {elapsed:.1f}s\")\n",
                "    metrics[\"tag\"] = tag\n",
                "    return metrics\n",
                "\n",
                "\n",
                "print(\"\u2713 Evaluation helpers defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5 \u2013 Baseline evaluation (Qwen3-Embedding-0.6B, no fine-tuning)\n",
                "print(f\"Loading base model: {BASE_MODEL} ...\")\n",
                "base_model = SentenceTransformer(BASE_MODEL, trust_remote_code=True)\n",
                "base_model.to(DEVICE)\n",
                "\n",
                "baseline_metrics = evaluate_model(base_model, test_ds, tag=\"baseline (Qwen3-Embedding-0.6B)\")\n",
                "\n",
                "# Free GPU memory\n",
                "del base_model\n",
                "torch.cuda.empty_cache()\n",
                "print(\"\\n\u2713 GPU memory freed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6 \u2013 Fine-tune with MultipleNegativesRankingLoss\n",
                "#\n",
                "# MultipleNegativesRankingLoss treats every other example in the batch\n",
                "# as a hard negative \u2013 no explicit negatives needed.\n",
                "# The model learns: encode_query(q) should be close to encode_document(code)\n",
                "\n",
                "print(f\"Loading model for fine-tuning: {BASE_MODEL} ...\")\n",
                "ft_model = SentenceTransformer(BASE_MODEL, trust_remote_code=True)\n",
                "ft_model.to(DEVICE)\n",
                "\n",
                "# Loss\n",
                "train_loss = losses.MultipleNegativesRankingLoss(\n",
                "    model=ft_model,\n",
                "    scale=20.0,\n",
                "    similarity_fct=losses.util.cos_sim,\n",
                ")\n",
                "\n",
                "# Training arguments\n",
                "training_args = SentenceTransformerTrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LR,\n",
                "    warmup_ratio=WARMUP_RATIO,\n",
                "    fp16=True,\n",
                "    logging_steps=50,\n",
                "    save_strategy=\"no\",\n",
                "    report_to=\"none\",\n",
                "    multi_dataset_batch_sampler=\"round_robin\",\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "trainer = SentenceTransformerTrainer(\n",
                "    model=ft_model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    loss=train_loss,\n",
                ")\n",
                "\n",
                "print(f\"\u2713 Training on {len(train_ds)} pairs, {EPOCHS} epochs, batch={BATCH_SIZE}\")\n",
                "print(\"  Loss: MultipleNegativesRankingLoss (scale=20, in-batch negatives)\")\n",
                "print(\"\\nStarting training...\")\n",
                "t0 = time.time()\n",
                "trainer.train()\n",
                "print(f\"\\n\u2713 Training complete in {time.time()-t0:.1f}s\")\n",
                "\n",
                "# Save\n",
                "ft_model.save(OUTPUT_DIR)\n",
                "print(f\"\u2713 Model saved to {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7 \u2013 Post fine-tuning evaluation\n",
                "postsft_metrics = evaluate_model(ft_model, test_ds, tag=\"post fine-tuning\")\n",
                "\n",
                "# Also evaluate the uploaded HF model for reference\n",
                "print(f\"\\nLoading uploaded HF model: {FINETUNED_HF} ...\")\n",
                "try:\n",
                "    hf_model = SentenceTransformer(FINETUNED_HF, trust_remote_code=True)\n",
                "    hf_model.to(DEVICE)\n",
                "    hf_metrics = evaluate_model(hf_model, test_ds, tag=f\"HF model ({FINETUNED_HF})\")\n",
                "    del hf_model\n",
                "    torch.cuda.empty_cache()\n",
                "except Exception as e:\n",
                "    print(f\"  (Could not load HF model: {e})\")\n",
                "    hf_metrics = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8 \u2013 Final comparison & failure analysis\n",
                "b = baseline_metrics\n",
                "a = postsft_metrics\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"  FINAL COMPARISON\")\n",
                "print(\"=\"*60)\n",
                "print(f\"  {'Metric':<12}  {'Baseline':>10}  {'Post-SFT':>10}  {'Delta':>8}\")\n",
                "print(f\"  {'-'*12}  {'-'*10}  {'-'*10}  {'-'*8}\")\n",
                "for metric in [\"MRR@10\", \"nDCG@10\", \"Recall@10\"]:\n",
                "    bv = b[metric]; av = a[metric]; dv = av - bv\n",
                "    arrow = \"\u2191\" if dv > 0 else (\"\u2193\" if dv < 0 else \"=\")\n",
                "    print(f\"  {metric:<12}  {bv:>10.4f}  {av:>10.4f}  {dv:>+7.4f} {arrow}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Save metrics JSON\n",
                "results = {\"baseline\": b, \"post_sft\": a}\n",
                "if hf_metrics:\n",
                "    results[\"hf_model\"] = hf_metrics\n",
                "with open(\"/content/track_c_metrics.json\", \"w\") as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "print(\"\\n\u2713 Metrics saved to /content/track_c_metrics.json\")\n",
                "\n",
                "# ---- Failure Analysis ----\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"  FAILURE ANALYSIS (queries where baseline rank > 5)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "queries, corpus, relevant = build_retrieval_corpus(test_ds)\n",
                "qids   = list(queries.keys())\n",
                "dids   = list(corpus.keys())\n",
                "qtexts = [queries[q] for q in qids]\n",
                "dtexts = [corpus[d]  for d in dids]\n",
                "\n",
                "# Re-encode with both models for comparison\n",
                "base_model2 = SentenceTransformer(BASE_MODEL, trust_remote_code=True)\n",
                "base_model2.to(DEVICE)\n",
                "\n",
                "if hasattr(base_model2, \"encode_query\"):\n",
                "    bq = base_model2.encode_query(qtexts,  show_progress_bar=False)\n",
                "    bd = base_model2.encode_document(dtexts, show_progress_bar=False)\n",
                "    aq = ft_model.encode_query(qtexts,  show_progress_bar=False)\n",
                "    ad = ft_model.encode_document(dtexts, show_progress_bar=False)\n",
                "else:\n",
                "    bq = base_model2.encode(qtexts,  normalize_embeddings=True, show_progress_bar=False)\n",
                "    bd = base_model2.encode(dtexts,  normalize_embeddings=True, show_progress_bar=False)\n",
                "    aq = ft_model.encode(qtexts,  normalize_embeddings=True, show_progress_bar=False)\n",
                "    ad = ft_model.encode(dtexts,  normalize_embeddings=True, show_progress_bar=False)\n",
                "\n",
                "del base_model2\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "b_sims = np.dot(bq, bd.T)\n",
                "a_sims = np.dot(aq, ad.T)\n",
                "\n",
                "failures = []\n",
                "for qi, qid in enumerate(qids):\n",
                "    rel_dids = relevant[qid]\n",
                "    b_ranked = np.argsort(-b_sims[qi])\n",
                "    a_ranked = np.argsort(-a_sims[qi])\n",
                "\n",
                "    b_rank = next((r+1 for r, di in enumerate(b_ranked) if dids[di] in rel_dids), 999)\n",
                "    a_rank = next((r+1 for r, di in enumerate(a_ranked) if dids[di] in rel_dids), 999)\n",
                "\n",
                "    if b_rank > 5:\n",
                "        failures.append({\n",
                "            \"query\":    queries[qid],\n",
                "            \"b_rank\":   b_rank,\n",
                "            \"a_rank\":   a_rank,\n",
                "            \"improved\": a_rank < b_rank,\n",
                "        })\n",
                "\n",
                "print(f\"  Found {len(failures)} hard queries (baseline rank > 5)\")\n",
                "for i, f in enumerate(failures[:15], 1):\n",
                "    arrow = \"\u2191 improved\" if f[\"improved\"] else (\"\u2193 worse\" if f[\"a_rank\"] > f[\"b_rank\"] else \"= same\")\n",
                "    print(f\"  [{i:2d}] Baseline rank {f['b_rank']:3d} \u2192 FT rank {f['a_rank']:3d}  {arrow}\")\n",
                "    print(f\"       Query: {f['query'][:90]}\")\n",
                "\n",
                "print(\"\\n\u2713 Track C complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9 \u2013 [Optional] Upload fine-tuned model to Hugging Face\n",
                "# Uncomment and set your HF token to push the model\n",
                "\n",
                "# from huggingface_hub import login\n",
                "# login(token=\"hf_YOUR_TOKEN_HERE\")\n",
                "# ft_model.push_to_hub(\"YOUR_HF_USERNAME/track_c_embedding_finetuned\")\n",
                "# print(\"\u2713 Model pushed to Hugging Face Hub\")\n",
                "\n",
                "print(\"Skipping upload (uncomment above to push to HF Hub)\")"
            ]
        }
    ]
}