"""
Dual-Repo Data Preparation for Repo-Specific Pretraining
========================================================
Builds cleaned code corpora for:
  1) HyperSwitch (Rust)
  2) verl (Python)
"""

import argparse
import ast
import json
import os
import random
import re
import shutil
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List


@dataclass(frozen=True)
class RepoSpec:
    name: str
    url: str
    repo_dir: str
    corpus_dir: str
    language: str
    extension: str
    min_lines: int
    max_lines: int
    max_files: int
    include_path_prefixes: tuple
    skip_path_tokens: tuple


SPECS: Dict[str, RepoSpec] = {
    "hyperswitch": RepoSpec(
        name="hyperswitch",
        url="https://github.com/juspay/hyperswitch.git",
        repo_dir="data/hyperswitch_repo",
        corpus_dir="data/code_corpus_hyperswitch",
        language="rust",
        extension=".rs",
        min_lines=25,
        max_lines=4000,
        max_files=300,
        include_path_prefixes=("crates/",),
        skip_path_tokens=(
            ".git",
            "target",
            "tests",
            "test",
            "examples",
            "docs",
            "api-reference",
            "migrations",
            "v2_migrations",
            "v2_compatible_migrations",
            "cypress-tests",
            "loadtest",
            "monitoring",
            "scripts",
        ),
    ),
    "verl": RepoSpec(
        name="verl",
        url="https://github.com/volcengine/verl.git",
        repo_dir="data/verl_repo",
        corpus_dir="data/code_corpus_verl",
        language="python",
        extension=".py",
        min_lines=20,
        max_lines=3500,
        max_files=300,
        include_path_prefixes=("verl/",),
        skip_path_tokens=(
            ".git",
            ".venv",
            "__pycache__",
            "tests",
            "test",
            "docs",
            "examples",
            "build",
            "dist",
            "benchmark",
            "scripts",
            "experimental",
            "playground",
            "tools",
        ),
    ),
}

RANDOM_SEED = 42
GEN_MARKERS = (
    "generated by",
    "auto-generated",
    "autogenerated",
    "do not edit",
    "@generated",
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--repos",
        choices=["both", "hyperswitch", "verl"],
        default="both",
        help="Which repo corpus to build.",
    )
    parser.add_argument(
        "--refresh",
        action="store_true",
        help="If set, hard reset and pull latest for existing repos.",
    )
    parser.add_argument("--seed", type=int, default=RANDOM_SEED)
    return parser.parse_args()


def clone_or_refresh_repo(spec: RepoSpec, refresh: bool) -> None:
    repo_path = Path(spec.repo_dir)
    if not repo_path.exists():
        print(f"[INFO] Cloning {spec.url} -> {spec.repo_dir}")
        subprocess.run(["git", "clone", "--depth", "1", spec.url, spec.repo_dir], check=True)
        return

    print(f"[INFO] Repo already exists: {spec.repo_dir}")
    if refresh:
        print(f"[INFO] Refreshing {spec.name} repo")
        subprocess.run(["git", "-C", spec.repo_dir, "fetch", "--depth", "1", "origin"], check=True)
        subprocess.run(["git", "-C", spec.repo_dir, "reset", "--hard", "origin/HEAD"], check=True)


def should_skip_path(rel_path: str, spec: RepoSpec) -> bool:
    parts = set(rel_path.split("/"))
    if any(token in parts or token in rel_path for token in spec.skip_path_tokens):
        return True

    if spec.include_path_prefixes and not any(
        rel_path.startswith(prefix) for prefix in spec.include_path_prefixes
    ):
        return True
    return False


def seems_generated(content: str) -> bool:
    head = "\n".join(content.splitlines()[:30]).lower()
    return any(marker in head for marker in GEN_MARKERS)


def rust_features(content: str) -> Dict[str, int]:
    num_functions = len(
        re.findall(r"(?m)^\s*(?:pub\s+)?(?:async\s+)?fn\s+[A-Za-z_]\w*", content)
    )
    num_types = len(
        re.findall(r"(?m)^\s*(?:pub\s+)?(?:struct|enum|trait|impl)\s+[A-Za-z_]\w*", content)
    )
    return {"num_functions": num_functions, "num_types": num_types}


def python_features(content: str) -> Dict[str, int]:
    tree = ast.parse(content)
    num_functions = 0
    num_classes = 0
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            num_functions += 1
        elif isinstance(node, ast.ClassDef):
            num_classes += 1
    return {"num_functions": num_functions, "num_types": num_classes}


def collect_code_files(spec: RepoSpec) -> List[Dict]:
    repo_path = Path(spec.repo_dir)
    if not repo_path.exists():
        raise FileNotFoundError(f"Repo missing: {spec.repo_dir}")

    candidates: List[Dict] = []
    for file_path in sorted(repo_path.rglob(f"*{spec.extension}")):
        rel_path = str(file_path.relative_to(repo_path))
        if should_skip_path(rel_path, spec):
            continue

        try:
            content = file_path.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue

        if not content.strip() or seems_generated(content):
            continue

        num_lines = len(content.splitlines())
        if num_lines < spec.min_lines or num_lines > spec.max_lines:
            continue

        try:
            feats = rust_features(content) if spec.language == "rust" else python_features(content)
        except Exception:
            # Invalid python / parse edge case => skip noisy file
            continue

        if feats["num_functions"] + feats["num_types"] < 2:
            continue

        size_bytes = len(content.encode("utf-8"))
        score = (
            feats["num_functions"] * 3
            + feats["num_types"] * 4
            + min(num_lines // 40, 40)
            + min(size_bytes // 1200, 40)
        )

        candidates.append(
            {
                "rel_path": rel_path,
                "abs_path": str(file_path),
                "num_lines": num_lines,
                "num_functions": feats["num_functions"],
                "num_types": feats["num_types"],
                "size_bytes": size_bytes,
                "quality_score": score,
            }
        )

    # Select most useful files (high-signal first), then deterministic order.
    candidates = sorted(candidates, key=lambda x: (-x["quality_score"], x["rel_path"]))
    selected = candidates[: spec.max_files]
    selected = sorted(selected, key=lambda x: x["rel_path"])
    return selected


def write_corpus(file_metas: List[Dict], corpus_dir: str) -> None:
    corpus_path = Path(corpus_dir)
    if corpus_path.exists():
        shutil.rmtree(corpus_path)
    corpus_path.mkdir(parents=True, exist_ok=True)

    for meta in file_metas:
        src = Path(meta["abs_path"])
        safe_name = meta["rel_path"].replace("/", "__")
        dst = corpus_path / safe_name
        shutil.copy2(src, dst)
        meta["corpus_filename"] = safe_name


def build_repo_corpus(spec: RepoSpec, refresh: bool) -> Dict:
    clone_or_refresh_repo(spec, refresh=refresh)
    print(f"[INFO] Collecting filtered files for {spec.name}...")
    files = collect_code_files(spec)
    print(f"[INFO] Selected {len(files)} files for {spec.name}")

    write_corpus(files, spec.corpus_dir)
    metadata = {
        "repo_name": spec.name,
        "repo_url": spec.url,
        "repo_dir": spec.repo_dir,
        "corpus_dir": spec.corpus_dir,
        "language": spec.language,
        "extension": spec.extension,
        "total_files": len(files),
        "total_lines": sum(f["num_lines"] for f in files),
        "total_functions": sum(f["num_functions"] for f in files),
        "total_types": sum(f["num_types"] for f in files),
        "files": files,
    }
    metadata_path = f"data/corpus_metadata_{spec.name}.json"
    with open(metadata_path, "w", encoding="utf-8") as handle:
        json.dump(metadata, handle, indent=2)
    print(f"[INFO] Metadata saved: {metadata_path}")
    return metadata


def main() -> None:
    args = parse_args()
    random.seed(args.seed)

    os.makedirs("data", exist_ok=True)
    os.makedirs("results", exist_ok=True)

    selected = ["hyperswitch", "verl"] if args.repos == "both" else [args.repos]
    summaries = {}
    for repo_name in selected:
        spec = SPECS[repo_name]
        summaries[repo_name] = build_repo_corpus(spec, refresh=args.refresh)

    summary_path = "data/corpus_metadata_both.json" if len(selected) == 2 else "data/corpus_metadata.json"
    with open(summary_path, "w", encoding="utf-8") as handle:
        json.dump(summaries, handle, indent=2)

    print("\n" + "=" * 70)
    print("DATASET PREPARATION SUMMARY")
    print("=" * 70)
    for name in selected:
        item = summaries[name]
        print(
            f"  {name:<12} files={item['total_files']:<4} "
            f"lines={item['total_lines']:<8} corpus={item['corpus_dir']}"
        )
    print("=" * 70)
    print(f"[INFO] Combined summary saved: {summary_path}")


if __name__ == "__main__":
    main()
