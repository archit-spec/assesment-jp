{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df655a73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "OUTPUT_DIR = Path(\"results/track_c\")\n",
    "\n",
    "\n",
    "def compute_metrics(query_embeddings, corpus_embeddings, relevant_indices, k=10):\n",
    "    \"\"\"Compute MRR@k, nDCG@k, Recall@k given query and corpus embeddings.\"\"\"\n",
    "    # Cosine similarity: normalize then dot product\n",
    "    query_norm = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "    corpus_norm = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "    scores = query_norm @ corpus_norm.T  # (num_queries, num_corpus)\n",
    "\n",
    "    mrr_scores = []\n",
    "    ndcg_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for i, rel_idx in enumerate(relevant_indices):\n",
    "        ranked = np.argsort(-scores[i])[:k]\n",
    "\n",
    "        # MRR@k\n",
    "        mrr = 0.0\n",
    "        for rank, idx in enumerate(ranked):\n",
    "            if idx == rel_idx:\n",
    "                mrr = 1.0 / (rank + 1)\n",
    "                break\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "        # nDCG@k\n",
    "        dcg = 0.0\n",
    "        idcg = 1.0  # single relevant doc, ideal DCG = 1/log2(2) = 1\n",
    "        for rank, idx in enumerate(ranked):\n",
    "            if idx == rel_idx:\n",
    "                dcg = 1.0 / np.log2(rank + 2)\n",
    "                break\n",
    "        ndcg_scores.append(dcg / idcg)\n",
    "\n",
    "        # Recall@k\n",
    "        recall_scores.append(1.0 if rel_idx in ranked else 0.0)\n",
    "\n",
    "    return {\n",
    "        f\"MRR@{k}\": np.mean(mrr_scores),\n",
    "        f\"nDCG@{k}\": np.mean(ndcg_scores),\n",
    "        f\"Recall@{k}\": np.mean(recall_scores),\n",
    "    }\n",
    "\n",
    "\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "def load_model(model_id, device):\n",
    "    kwargs = {}\n",
    "    if device == \"cuda\":\n",
    "        kwargs[\"model_kwargs\"] = {\"torch_dtype\": torch.float16}\n",
    "    return SentenceTransformer(model_id, device=device, trust_remote_code=True, **kwargs)\n",
    "\n",
    "\n",
    "def eval_model(model, corpus_unique, all_queries, relevant_indices):\n",
    "    with torch.no_grad():\n",
    "        corpus_embeddings = model.encode(\n",
    "            corpus_unique, batch_size=8, show_progress_bar=True, convert_to_numpy=True\n",
    "        )\n",
    "        query_embeddings = model.encode(\n",
    "            all_queries, batch_size=8, show_progress_bar=True, convert_to_numpy=True\n",
    "        )\n",
    "    metrics = compute_metrics(query_embeddings, corpus_embeddings, relevant_indices, k=10)\n",
    "    del corpus_embeddings, query_embeddings\n",
    "    free_memory()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_model_id = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "    finetuned_model_id = \"archit11/assesment_qwen3_embedding_06b_e3\"\n",
    "    dataset_id = \"archit11/assesment_embeddings_new\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(f\"Loading dataset {dataset_id}...\")\n",
    "    dataset = load_dataset(dataset_id, split=\"test\")\n",
    "    print(f\"Loaded {len(dataset)} examples | Columns: {dataset.column_names}\")\n",
    "\n",
    "    # corpus = anchors (code); queries come from the queries list field\n",
    "    corpus_unique = list(dataset[\"anchor\"])\n",
    "    all_queries = []\n",
    "    relevant_indices = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        for q in row[\"queries\"]:\n",
    "            all_queries.append(q)\n",
    "            relevant_indices.append(i)\n",
    "    print(f\"Corpus size: {len(corpus_unique)}, Total queries: {len(all_queries)}\")\n",
    "\n",
    "    # Baseline\n",
    "    print(f\"\\nEvaluating base model: {base_model_id}...\")\n",
    "    base_model = load_model(base_model_id, device)\n",
    "    baseline = eval_model(base_model, corpus_unique, all_queries, relevant_indices)\n",
    "    del base_model\n",
    "    free_memory()\n",
    "\n",
    "    # Fine-tuned\n",
    "    print(f\"\\nEvaluating fine-tuned model: {finetuned_model_id}...\")\n",
    "    ft_model = load_model(finetuned_model_id, device)\n",
    "    finetuned = eval_model(ft_model, corpus_unique, all_queries, relevant_indices)\n",
    "    del ft_model\n",
    "    free_memory()\n",
    "\n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"{'Metric':<15} {'Baseline':>10} {'Fine-Tuned':>12} {'Δ':>8}\")\n",
    "    print(\"-\" * 47)\n",
    "    for metric in baseline:\n",
    "        b, f = baseline[metric], finetuned[metric]\n",
    "        print(f\"{metric:<15} {b:>10.4f} {f:>12.4f} {f-b:>+8.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4jnsil64azu",
   "source": "### Expected Results\n\n| Metric | Baseline | Fine-Tuned | Δ |\n|--------|----------|------------|---|\n| MRR@10 | 0.8875 | **0.9617** | +0.0742 ↑ |\n| nDCG@10 | 0.9126 | **0.9710** | +0.0584 ↑ |\n| Recall@10 | 0.9903 | **1.0000** | +0.0097 ↑ |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "zkhhim7dkrb",
   "source": "# Track C – Embedding Retrieval Evaluation\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/archit-spec/assesment-jp/blob/main/eval_track_c.ipynb)\n\nEvaluates **baseline** (`Qwen/Qwen3-Embedding-0.6B`) vs **fine-tuned** (`archit11/assesment_qwen3_embedding_06b_e3`) retrieval metrics on the Hyperswitch code embedding dataset.\n\n**Dataset:** [`archit11/assesment_embeddings_new`](https://huggingface.co/datasets/archit11/assesment_embeddings_new)\n\n### Install dependencies\n```bash\npip install sentence-transformers datasets torch\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}