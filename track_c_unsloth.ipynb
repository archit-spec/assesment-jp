{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4",
            "name": "Track C \u2013 Unsloth Embeddings"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# \ud83e\udde1 Track C \u2013 Unsloth Embedding Fine-Tuning\n\nFine-tune `unsloth/Qwen3-Embedding-0.6B` using **FastSentenceTransformer** for memory efficiency on Colab T4.\n\n### Why Unsloth?\n- **30% less VRAM**, fits 2x larger batch sizes.\n- Supports LoRA for embedding models.\n\n---\n\n## \ud83d\udce6 Dataset: [`archit11/code-embedding-dataset`](https://huggingface.co/datasets/archit11/code-embedding-dataset)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 \u2013 Install Unsloth\n",
                "%%capture\n",
                "import os, re\n",
                "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
                "    !pip install unsloth\n",
                "else:\n",
                "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
                "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
                "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
                "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
                "!pip install transformers==4.56.2\n",
                "!pip install --no-deps trl==0.22.2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 \u2013 Load Model\n",
                "from unsloth import FastSentenceTransformer\n",
                "\n",
                "model = FastSentenceTransformer.from_pretrained(\n",
                "    model_name = \"unsloth/Qwen3-Embedding-0.6B\",\n",
                "    max_seq_length = 2048,\n",
                "    full_finetuning = False,\n",
                ")\n",
                "\n",
                "model = FastSentenceTransformer.get_peft_model(\n",
                "    model,\n",
                "    r = 32,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 32,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = False,\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,\n",
                "    loftq_config = None,\n",
                "    task_type = \"FEATURE_EXTRACTION\"\n",
                ")\n",
                "print(\"\u2713 Unsloth embedding model loaded with LoRA\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3 \u2013 Load Dataset\n",
                "from datasets import load_dataset\n",
                "ds = load_dataset(\"archit11/code-embedding-dataset\")\n",
                "train_ds = ds[\"train\"]\n",
                "test_ds  = ds[\"test\"] if \"test\" in ds else train_ds.select(range(len(train_ds)-24, len(train_ds)))\n",
                "print(f\"\u2713 Loaded {len(train_ds)} train pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4 \u2013 Train\n",
                "from sentence_transformers import (\n",
                "    SentenceTransformerTrainer,\n",
                "    SentenceTransformerTrainingArguments,\n",
                "    losses\n",
                ")\n",
                "from sentence_transformers.training_args import BatchSamplers\n",
                "from unsloth import is_bf16_supported\n",
                "\n",
                "loss = losses.MultipleNegativesRankingLoss(model)\n",
                "\n",
                "trainer = SentenceTransformerTrainer(\n",
                "    model = model,\n",
                "    train_dataset = train_ds,\n",
                "    loss = loss,\n",
                "    args = SentenceTransformerTrainingArguments(\n",
                "        output_dir = \"output_track_c\",\n",
                "        num_train_epochs = 2,\n",
                "        per_device_train_batch_size = 16,  # T4 safe with Unsloth\n",
                "        gradient_accumulation_steps = 1,\n",
                "        learning_rate = 3e-5,\n",
                "        fp16 = not is_bf16_supported(),\n",
                "        bf16 = is_bf16_supported(),\n",
                "        logging_steps = 50,\n",
                "        warmup_ratio = 0.03,\n",
                "        report_to = \"none\",\n",
                "        lr_scheduler_type = \"constant_with_warmup\",\n",
                "        batch_sampler = BatchSamplers.NO_DUPLICATES, # Important for MNRL\n",
                "    ),\n",
                ")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5 \u2013 Save & Evaluate (Placeholder)\n",
                "model.save_pretrained(\"output_track_c\")\n",
                "print(\"\u2713 Model saved. See standard Track C notebook for evaluation logic.\")"
            ]
        }
    ]
}