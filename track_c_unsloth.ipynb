{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Track C â€“ Unsloth Embedding Fine-Tuning\n",
                "\n",
                "Fine-tune `unsloth/Qwen3-Embedding-0.6B` using **FastSentenceTransformer** for memory efficiency on Colab T4.\n",
                "\n",
                "### Why Unsloth?\n",
                "- **30% less VRAM**, fits 2x larger batch sizes.\n",
                "- Supports LoRA for embedding models.\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“¦ Dataset: [`archit11/code-embedding-dataset`](https://huggingface.co/datasets/archit11/code-embedding-dataset)\n",
                "\n",
                "### ðŸ“Š Results (Best Run - 3 Epochs)\n",
                "\n",
                "| Metric | Baseline | Fine-Tuned | Î” |\n",
                "|--------|----------|------------|---|\n",
                "| **MRR@10** | 0.8840 | **0.9617** | **+0.0777 â†‘** |\n",
                "| **nDCG@10** | 0.9093 | **0.9710** | **+0.0617 â†‘** |\n",
                "| **Recall@10** | 0.9870 | **1.0000** | **+0.0130 â†‘** |\n",
                "\n",
                "> **Note**: Results based on 3 epochs, batch size 8, learning rate 2e-5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 â€“ Install Unsloth\n",
                "%%capture\n",
                "import os, re\n",
                "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
                "    !pip install unsloth\n",
                "else:\n",
                "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
                "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
                "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
                "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
                "!pip install transformers==4.56.2\n",
                "!pip install --no-deps trl==0.22.2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 â€“ Load Model\n",
                "from unsloth import FastSentenceTransformer\n",
                "\n",
                "model = FastSentenceTransformer.from_pretrained(\n",
                "    model_name = \"unsloth/Qwen3-Embedding-0.6B\",\n",
                "    max_seq_length = 2048,\n",
                "    full_finetuning = False,\n",
                ")\n",
                "\n",
                "model = FastSentenceTransformer.get_peft_model(\n",
                "    model,\n",
                "    r = 32,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 32,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = False,\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,\n",
                "    loftq_config = None,\n",
                "    task_type = \"FEATURE_EXTRACTION\"\n",
                ")\n",
                "print(\"âœ“ Unsloth embedding model loaded with LoRA\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3 â€“ Load Dataset\n",
                "from datasets import load_dataset\n",
                "ds = load_dataset(\"archit11/code-embedding-dataset\")\n",
                "train_ds = ds[\"train\"]\n",
                "test_ds  = ds[\"test\"] if \"test\" in ds else train_ds.select(range(len(train_ds)-24, len(train_ds)))\n",
                "print(f\"âœ“ Loaded {len(train_ds)} train pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4 â€“ Train\n",
                "from sentence_transformers import (\n",
                "    SentenceTransformerTrainer,\n",
                "    SentenceTransformerTrainingArguments,\n",
                "    losses\n",
                ")\n",
                "from sentence_transformers.training_args import BatchSamplers\n",
                "from unsloth import is_bf16_supported\n",
                "\n",
                "loss = losses.MultipleNegativesRankingLoss(model)\n",
                "\n",
                "trainer = SentenceTransformerTrainer(\n",
                "    model = model,\n",
                "    train_dataset = train_ds,\n",
                "    loss = loss,\n",
                "    args = SentenceTransformerTrainingArguments(\n",
                "        output_dir = \"output_track_c\",\n",
                "        num_train_epochs = 3,  # Best results from 3 epochs\n",
                "        per_device_train_batch_size = 16,  # T4 safe with Unsloth\n",
                "        gradient_accumulation_steps = 1,\n",
                "        learning_rate = 2e-5,\n",
                "        fp16 = not is_bf16_supported(),\n",
                "        bf16 = is_bf16_supported(),\n",
                "        logging_steps = 50,\n",
                "        warmup_ratio = 0.03,\n",
                "        report_to = \"none\",\n",
                "        lr_scheduler_type = \"constant_with_warmup\",\n",
                "        batch_sampler = BatchSamplers.NO_DUPLICATES, # Important for MNRL\n",
                "    ),\n",
                ")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5 â€“ Save & Evaluate (Placeholder)\n",
                "model.save_pretrained(\"output_track_c\")\n",
                "print(\"âœ“ Model saved. See standard Track C notebook for evaluation logic.\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "name": "Track C â€“ Unsloth Embeddings",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
