[
  {
    "query": "How does the launch_server_process function coordinate with the overall distributed training coordinator to ensure server processes are correctly placed on specific GPUs and cleaned up upon job termination?",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "Draw a mermaid sequence diagram showing the lifecycle of a DataProto object from its creation, through potential padding, distribution via DataProtoFuture chunks to Ray workers, and the final all_gather operation across the process group.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253
  },
  {
    "query": "During the launch_server_process readiness polling, if the server passes the health check but crashes immediately before the first inference request, does the adapter implement any liveness checks to detect this state before timing out?",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "Distributed tensor broadcasting across ranks",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448,
    "label_source": "synthetic_label"
  },
  {
    "query": "In the context of an RLHF training loop, how does the DataProtoFuture class facilitate asynchronous data loading or preprocessing, and at what point in the training iteration does the blocking 'get()' call typically occur?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253
  },
  {
    "query": "RLHF rollout worker communication interface",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954,
    "label_source": "synthetic_label"
  },
  {
    "query": "Base tokenizer class for hybrid engine",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163,
    "label_source": "synthetic_label"
  },
  {
    "query": "SGLang HTTP server engine adapter",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954,
    "label_source": "synthetic_label"
  },
  {
    "query": "PPO critic gradient accumulation optimization",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263,
    "label_source": "synthetic_label"
  },
  {
    "query": "How does this tokenizer interface facilitate data flow between the rollout workers and the vLLM inference engine, particularly regarding the requirement for property names to align with HF standards?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163
  },
  {
    "query": "Launch and manage SGLang server process",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954,
    "label_source": "synthetic_label"
  },
  {
    "query": "Distributed all gather data proto function",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253,
    "label_source": "synthetic_label"
  },
  {
    "query": "Load class from fully qualified name",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236,
    "label_source": "synthetic_label"
  },
  {
    "query": "Import external libraries dynamically",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236,
    "label_source": "synthetic_label"
  },
  {
    "query": "Reward scoring and request abort methods",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954,
    "label_source": "synthetic_label"
  },
  {
    "query": "In the branch where `self.config.flash_decode` is active during inference, `rotary_pos_emb` is explicitly set to `None`; how is positional encoding applied in this path if not through the standard RoPE mechanism, and does this rely on pre-embedded positions?",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "Asynchronous DataProtoFuture handling class",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253,
    "label_source": "synthetic_label"
  },
  {
    "query": "Draw a mermaid sequence diagram showing the interaction between Pipeline Parallel ranks and Rank 0 during the _broadcast_tp_shard_tensor calls within merge_megatron_ckpt_qwen2, highlighting where data converges.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "Merge Megatron checkpoint state dictionaries",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448,
    "label_source": "synthetic_label"
  },
  {
    "query": "Draw a mermaid diagram showing the inheritance relationship between `HybridEngineBaseTokenizer` and a hypothetical concrete implementation, highlighting how the abstract properties map to specific vLLM or Hugging Face tokenizer attributes.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163
  },
  {
    "query": "Abstract tokenizer interface for vLLM rollout",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163,
    "label_source": "synthetic_label"
  },
  {
    "query": "The dtype conversion happens at the very end (if dtype != v.dtype: state_dict[k] = v.to(dtype)). Since tensors are already gathered in dtype (likely FP16/BF16), what specific scenario requires this final cast, and does performing it after gathering unnecessarily spike peak memory usage on rank 0?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "In load_module, when loading from a file path without an explicit module_name, a hash-based name is generated. If the same file is loaded twice in the same process, does this guarantee the same module object is returned, or does it bypass sys.modules caching and re-execute the file every time?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "If bias=True is passed but skip_bias_add=True is also set, how does the parent ColumnParallelLinear handle the bias tensor registration versus the actual addition operation, and could this lead to unused parameters accumulating in the state dict?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79
  },
  {
    "query": "Why was a custom abstract base class `HybridEngineBaseTokenizer` created instead of directly using Hugging Face's `PreTrainedTokenizer` or `vLLM`'s native tokenizer interfaces, and what specific incompatibilities does this abstraction resolve?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163
  }
]