[
  {
    "query": "Qwen2 model parallel linear definitions",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79,
    "label_source": "synthetic_label"
  },
  {
    "query": "Does the fused_rms_norm_affine kernel from Apex support all dtype configurations used in this framework (e.g., bf16 vs fp16), and is there fallback logic if the fused kernel fails to launch on specific GPU architectures?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48
  },
  {
    "query": "In _megatron_calc_layer_map, the assertion num_layers_per_model * pp_size * virtual_pp_size == config.num_hidden_layers assumes perfect divisibility. If we change pipeline parallelism sizes dynamically without updating the config, will this fail gracefully or cause silent corruption during layer mapping?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "Check if packages are installed",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236,
    "label_source": "synthetic_label"
  },
  {
    "query": "Verify deep learning package dependencies",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236,
    "label_source": "synthetic_label"
  },
  {
    "query": "Given that RMSNorm typically operates independently across sequence dimensions, what specific gradient synchronization behavior is triggered by marking the weight parameter as sequence_parallel, and how does this align with Megatron's sequence parallelism strategy?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48
  },
  {
    "query": "What specific architectural constraints of Qwen2.5-VL necessitate overriding `apply_rotary_pos_emb_absolute` instead of the standard Megatron RoPE implementation, and does this impact compatibility with other Megatron-based models?",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "Qwen2.5 VL self attention implementation",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225,
    "label_source": "synthetic_label"
  },
  {
    "query": "The all_gather_data_proto function moves data to the current device ID before gathering and then moves it back; what are the potential race conditions or performance penalties if this function is called concurrently on multiple processes within the same node sharing GPU resources?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253
  },
  {
    "query": "QKV parallel linear layer implementation",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79,
    "label_source": "synthetic_label"
  },
  {
    "query": "In the context of RLHF, this merger likely converts Megatron sharded weights to HuggingFace format for inference or reference models. Does this run inline during PPO updates, and how do we mitigate the synchronization barrier (dist.barrier) impacting training throughput?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "What is the responsibility boundary between BasePPOCritic and this implementation regarding distributed communication primitives, and why was the decision made to handle padding removal logic inside the critic worker rather than the data loader?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "transformers version compatibility utility functions",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57,
    "label_source": "synthetic_label"
  },
  {
    "query": "Apex fused RMS norm affine",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48,
    "label_source": "synthetic_label"
  },
  {
    "query": "The dynamic batching path performs `squeeze(0).unsqueeze(1)` followed by `rearrange` on `core_attn_out`; does this sequence guarantee a contiguous memory layout required by `linear_proj`, or could it introduce stride issues that degrade performance?",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "Tokenizer vocabulary and special token properties",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163,
    "label_source": "synthetic_label"
  },
  {
    "query": "What is the strategic decision behind using a silent fallback (returning `False`) for missing transformer functions instead of raising an explicit error to force dependency upgrades?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57
  },
  {
    "query": "Why does the version comparison logic use inclusive bounds (`<=`) for both min and max versions, and are there known breaking changes in the `transformers` library that strictly require exclusive upper bounds to prevent runtime crashes?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57
  },
  {
    "query": "Map global layer index to local",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448,
    "label_source": "synthetic_label"
  },
  {
    "query": "Why was an HTTP-based adapter chosen over direct engine integration for the SGLang backend, considering the latency overhead this introduces during high-frequency RLHF rollout steps?",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "The class name implies Data Parallelism, but imports include FullyShardedDataParallel and FSDPModule. What is the actual distributed strategy implemented here, and how does it coordinate with the FSDP utilities imported?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "RLHF training framework attention module",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225,
    "label_source": "synthetic_label"
  },
  {
    "query": "Apply rotary pos emb absolute",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225,
    "label_source": "synthetic_label"
  },
  {
    "query": "Attention and MLP parallel projections",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79,
    "label_source": "synthetic_label"
  },
  {
    "query": "The `decode` method accepts `torch.Tensor` and `np.ndarray` inputs; what are the performance implications of handling these conversions inside the implementation versus enforcing strict input types at the call site?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163
  },
  {
    "query": "There are multiple ways to load classes here (load_module + getattr, load_extern_object, load_class_from_fqn). What is the recommended pattern for new components, and why do we maintain three seemingly overlapping abstractions?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "Why were these specific subclasses created instead of configuring megatron.core.tensor_parallel.ColumnParallelLinear directly, and what specific Qwen2 or RLHF training requirements necessitate this abstraction layer?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79
  },
  {
    "query": "RLHF worker tokenizer base definition",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163,
    "label_source": "synthetic_label"
  },
  {
    "query": "RLHF training data communication protocol",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253,
    "label_source": "synthetic_label"
  },
  {
    "query": "How does the ulysses_sequence_parallel_size configuration interact with the outer data parallelism managed by the trainer loop, and what guarantees exist for gradient synchronization across these hybrid parallel dimensions?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "Since neither class overrides the forward method, how are downstream modules expected to slice the concatenated output tensor using the stored q_output_size and kv_output_size attributes without explicit indexing logic in this file?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79
  },
  {
    "query": "The merge_megatron_ckpt_qwen2 function gathers the entire merged state_dict onto rank 0. For large models (e.g., 72B), this creates a significant memory bottleneck on a single GPU. Why was a distributed checkpoint saving strategy not chosen here, and does this limit the maximum model size we can support?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "Async HTTP request handling for inference",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954,
    "label_source": "synthetic_label"
  },
  {
    "query": "What is the architectural rationale for using a metaclass (_DataProtoConfigMeta) to manage global configuration flags like auto_padding via environment variables, rather than passing explicit configuration objects through the data pipeline?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253
  },
  {
    "query": "The file imports UpdateWeightsFromTensorReqInput and mentions distributed weight updates in the header, but the visible methods focus on inference; how are weight synchronization triggers coordinated between the trainer and this rollout engine?",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "In union_tensor_dict, the code asserts that overlapping keys must have equal values; given that this operates on potentially large GPU tensors, what is the performance cost of the .equal() check, and could this become a bottleneck if called frequently during batch construction?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253
  },
  {
    "query": "Draw a mermaid diagram showing the control flow of the `is_transformers_version_in_range` function, including the package metadata retrieval, version parsing, conditional bound checks, and the caching mechanism.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57
  },
  {
    "query": "How does the output_size calculation in QKVParallelLinear account for tensor parallelism world size when num_key_value_heads is not evenly divisible by the number of GPUs, particularly for Grouped Query Attention scenarios?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79
  },
  {
    "query": "Draw a mermaid sequence diagram showing the lifecycle interaction between the parent process, the spawned SGLang server process, and the health check polling logic within launch_server_process.",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "fallback for missing flash attention function",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57,
    "label_source": "synthetic_label"
  },
  {
    "query": "Given that RLHF workloads often run in distributed environments with strict reproducibility requirements, how does the dynamic version checking in this module impact deployment consistency across different cluster nodes?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57
  },
  {
    "query": "The file manually calculates global ranks and layer mappings (_megatron_calc_global_rank, _megatron_calc_layer_map) instead of using Megatron's native checkpoint utilities. What specific limitations in the native API necessitated this custom implementation for Qwen2?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "vLLM compatible tokenizer abstraction layer",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163,
    "label_source": "synthetic_label"
  },
  {
    "query": "FSDP distributed critic model training",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263,
    "label_source": "synthetic_label"
  },
  {
    "query": "TensorDict based batch management system",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253,
    "label_source": "synthetic_label"
  },
  {
    "query": "Why does this module depend on both transformers.Qwen2Config and megatron.core.ModelParallelConfig instead of a unified configuration object, and how does this dual-dependency impact maintainability when upgrading underlying libraries?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48
  },
  {
    "query": "cached transformers version verification function",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57,
    "label_source": "synthetic_label"
  },
  {
    "query": "Qwen2 model checkpoint saving utilities",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448,
    "label_source": "synthetic_label"
  },
  {
    "query": "Draw a mermaid diagram showing the class inheritance hierarchy and the external library dependencies (Apex, Megatron, Transformers) invoked during the __init__ and forward methods of ParallelQwen2RMSNorm.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48
  },
  {
    "query": "Base data transfer protocol implementation",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253,
    "label_source": "synthetic_label"
  },
  {
    "query": "In _read_async_response, generic Exception handling swallows specific aiohttp errors; how does the calling code distinguish between retryable network timeouts and fatal server errors when parsing the response?",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "The docstring states \"package availability won't change during runtime,\" but in distributed RLHF (e.g., Ray actors), different workers might have different environments. Does caching these checks globally via @cache risk masking environment inconsistencies across cluster nodes?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "The constant DEFAULT_MAX_CONNECTIONS is set to 2000; is this limit enforced per AsyncHttpServerAdapter instance or globally per process, and what safeguards exist against exhausting system file descriptors during massive parallel rollout generation?",
    "code": "# Copyright 2025 z.ai\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file is adapted from multiple sources:\n# 1. THUDM/slime project\n#    Original source: https://github.com/THUDM/slime/blob/main/slime/backends/sglang_utils/http_server_engine.py\n#    Copyright 2025 z.ai\n#    Licensed under the Apache License, Version 2.0\n# 2. SGLang project\n#    Original source: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server_engine.py\n#    Copyright 2023-2024 SGLang Team\n#    Licensed under the Apache License, Version 2.0\n#\n# Modifications made by z.ai and ModelBest Inc. include but are not limited to:\n# - Enhanced error handling and retry logic\n# - Added async support with connection pooling\n# - Extended fun\n\n... [truncated 37642 chars] ...\n\nn await self.reward_score(\n            prompt=prompt,\n            input_ids=input_ids,\n            image_data=image_data,\n            lora_path=lora_path,\n        )\n\n    async def abort_request(self, rid: str = \"\", abort_all: bool = False) -> dict[str, Any]:\n        \"\"\"Abort a request asynchronously.\n\n        Args:\n            rid (str): The ID of the request to abort\n            abort_all (bool, optional): Whether to abort all requests. Defaults to False.\n\n        Returns:\n            Dict[str, Any]: Server response indicating abort status\n        \"\"\"\n        return await self._make_async_request(\"abort_request\", {\"rid\": rid, \"abort_all\": abort_all})\n",
    "function_name": "verl__workers__rollout__sglang_rollout__http_server_engine",
    "file": "verl__workers__rollout__sglang_rollout__http_server_engine.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 954
  },
  {
    "query": "The `is_transformers_version_in_range` function uses `lru_cache` without arguments; how does this behave if the underlying `transformers` package is hot-reloaded or if multiple virtual environments are somehow interleaved in a long-running process?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57
  },
  {
    "query": "Column parallel linear inheritance classes",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79,
    "label_source": "synthetic_label"
  },
  {
    "query": "Megatron core transformer attention override",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225,
    "label_source": "synthetic_label"
  },
  {
    "query": "Packed sequence params handling attention",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225,
    "label_source": "synthetic_label"
  },
  {
    "query": "Megatron tensor parallel linear layers",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79,
    "label_source": "synthetic_label"
  },
  {
    "query": "RLHF framework checkpoint saver utility",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448,
    "label_source": "synthetic_label"
  },
  {
    "query": "Calculate Megatron global rank mapping",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448,
    "label_source": "synthetic_label"
  },
  {
    "query": "PyTorch neural network RMSNorm module",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48,
    "label_source": "synthetic_label"
  },
  {
    "query": "How does this compatibility layer integrate with the broader verl training loop, and which specific components rely on the `flash_attn_supports_top_left_mask` feature toggle?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57
  },
  {
    "query": "DataProto batch data structure definition",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253,
    "label_source": "synthetic_label"
  },
  {
    "query": "Draw a mermaid diagram showing the control flow of the load_module function, specifically highlighting the branching logic between pkg:// and file:// prefixes and how sys.modules is handled.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "HybridEngineBaseTokenizer abstract base class",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163,
    "label_source": "synthetic_label"
  },
  {
    "query": "How does the DataProto abstraction bridge the gap between Ray's distributed task execution model and PyTorch's distributed data parallel requirements, specifically regarding the separation of tensor and non-tensor data?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement base data transfer protocol between any two functions, modules.\nWe can subclass Protocol to define more detailed batch info with specific keys\n\"\"\"\n\nimport contextlib\nimport copy\nimport logging\nimport math\nimport os\nimport pickle\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nimport ray\nimport tensordict\nimport torch\nimport torch.distributed\nfrom packaging import version\nfrom packaging.version import parse as parse_version\nfrom tensordict import TensorDict\nfrom torch.utils.data import DataLoader\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.py_functional import union_two_dict\nfrom verl.utils.torch_functional import allgather_dict\n\n... [truncated 46471 chars] ...\n\nibuted.all_gather\n    group_size = torch.distributed.get_world_size(group=process_group)\n    assert isinstance(data, DataProto)\n    prev_device = data.batch.device\n    data = data.to(get_device_id())\n    data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0)\n    data = data.to(prev_device)\n    # all gather non_tensor_batch\n    all_non_tensor_batch = [None for _ in range(group_size)]\n    torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group)\n    data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch}\n",
    "function_name": "verl__protocol",
    "file": "verl__protocol.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 1253
  },
  {
    "query": "Flash attention decode and prefill",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225,
    "label_source": "synthetic_label"
  },
  {
    "query": "In _forward_micro_batch, position_ids are transposed if dim == 3 specifically for qwen2vl mrope. How is this dimensionality check validated across different model architectures to prevent silent shape mismatches during rotary embedding alignment?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "Critic value prediction and clipping",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263,
    "label_source": "synthetic_label"
  },
  {
    "query": "The is_<package>_available functions catch ModuleNotFoundError during find_spec, but importlib.util.find_spec typically returns None rather than raising that error. Is this exception handling actually necessary, or does it mask underlying ImportError causes that we should see during startup?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "What is the architectural role of this module within the broader 'hybrid engine' concept, and how does it decouple the tokenization logic from the specific model backend used during RLHF training?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163
  },
  {
    "query": "The code asserts `rotary_pos_cos` and `rotary_pos_sin` are None unless in the flash decode path; what prevents upstream components from inadvertently passing these arguments during training, and is there a fallback mechanism if this assertion fails in production?",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "The docstring for `get_added_vocab` mentions a known issue where tokens are added even if they already exist; how should a concrete implementation handle this idempotency problem to prevent vocabulary bloat or index collisions?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThe base tokenizer class, required for any hybrid engine based rollout or inference with vLLM.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n__all__ = [\"HybridEngineBaseTokenizer\"]\n\n\nclass HybridEngineBaseTokenizer(ABC):\n    \"\"\"the tokenizer property and function name should align with HF's to meet vllm requirement\"\"\"\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        \"\"\"\n        `int`: Size of the base vocabulary (without the added tokens).\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def pad_token_id(self):\n        \"\"\"\n        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n        \"\"\"\n        pass\n\n   \n\n... [truncated 3640 chars] ...\n\nocabulary. This is\n        something we should change.\n\n        Returns:\n            `Dict[str, int]`: The added tokens.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n        \"\"\"\n        Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n        often want to remove sub-word tokenization artifacts at the same time.\n\n        Args:\n            tokens (`List[str]`): The token to join in a string.\n\n        Returns:\n            `str`: The joined tokens.\n        \"\"\"\n        pass\n\n    @property\n    def is_fast(self):\n        return False\n",
    "function_name": "verl__workers__rollout__tokenizer",
    "file": "verl__workers__rollout__tokenizer.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 163
  },
  {
    "query": "Cached package availability check functions",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236,
    "label_source": "synthetic_label"
  },
  {
    "query": "In the context of the larger verl framework, how do the gather_output and skip_bias_add flags influence communication overhead during the PPO training loop, and why are these exposed at this layer rather than handled globally?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79
  },
  {
    "query": "Given this is part of an RLHF framework, how does the choice between `checkpointed_attention_forward` and standard `core_attention` affect gradient computation overhead versus memory usage during the rollout phase of PPO/GRPO algorithms?",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "This module seems to be the gateway for plugin-style architecture (loading custom datasets/models via file://). How does this dynamic loading interact with the framework's serialization requirements (e.g., pickle) when passing objects between workers?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "Handle optional dependency imports",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236,
    "label_source": "synthetic_label"
  },
  {
    "query": "Data parallel PPO critic worker",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263,
    "label_source": "synthetic_label"
  },
  {
    "query": "Qwen2 RMSNorm layer implementation",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48,
    "label_source": "synthetic_label"
  },
  {
    "query": "Draw a mermaid flowchart showing the control flow within _forward_micro_batch, specifically detailing the conditional paths for use_remove_padding and ulysses_sequence_parallel_size.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "The torch.autocast context manager hardcodes dtype=torch.bfloat16. How does this behave on GPUs that do not support bfloat16, and why is this precision setting not configurable via the config object like use_remove_padding?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "hugging face transformers dependency management",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57,
    "label_source": "synthetic_label"
  },
  {
    "query": "Megatron sequence parallel normalization",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48,
    "label_source": "synthetic_label"
  },
  {
    "query": "The loss scaling factor logic branches on use_dynamic_bsz. What is the mathematical justification for scaling by response_mask.shape[0] / self.config.ppo_mini_batch_size versus 1 / self.gradient_accumulation, and how does this impact gradient stability when batch sizes fluctuate?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263
  },
  {
    "query": "Merged column parallel linear layer",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79,
    "label_source": "synthetic_label"
  },
  {
    "query": "The logic for reward_head checks getattr(gpt_model_module, 'reward_weight', None). If the attribute naming convention changes in the underlying model definition, this silent check might skip saving the reward head without raising an error. Should this be an explicit assertion instead?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom megatron.core import mpu\nfrom megatron.core.distributed import DistributedDataParallel as LocalDDP\nfrom megatron.core.transformer.module import Float16Module\nfrom torch.nn.parallel import DistributedDataParallel as torchDDP\n\nfrom verl.utils.device import get_device_id, get_torch_device\nfrom verl.utils.logger import print_rank_0\nfrom verl.utils.megatron_utils import unwrap_model\n\n\ndef _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):\n    \"\"\"given TP,DP,PP rank to get the global rank.\"\"\"\n\n    tp_size = mpu.get_tensor_model_parallel_world_size()\n    dp_size = mpu.get_data_parallel_world_size()\n    pp_size = mpu.get_pipeline_model_parallel\n\n... [truncated 16153 chars] ...\n\n                  src_pp_rank=pp_size - 1,\n                )\n\n            else:\n                _broadcast_tp_shard_tensor(\n                    getattr(gpt_model_module.lm_head, \"weight\", None) if pp_rank == pp_size - 1 else None,\n                    \"lm_head.weight\",\n                    src_pp_rank=pp_size - 1,\n                )\n\n    dist.barrier()\n\n    get_torch_device().empty_cache()\n    if torch.distributed.get_rank() == 0:\n        for k, v in state_dict.items():\n            if dtype != v.dtype:\n                state_dict[k] = v.to(dtype)\n\n    print_rank_0(f\"merge megatron ckpt done, time elapsed {time.time() - start_time}s\")\n    return state_dict\n",
    "function_name": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver",
    "file": "verl__models__qwen2__megatron__checkpoint_utils__qwen2_saver.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 448
  },
  {
    "query": "Multiprocess PPO critic implementation",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263,
    "label_source": "synthetic_label"
  },
  {
    "query": "check transformers library version range",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCompatibility utilities for different versions of transformers library.\n\"\"\"\n\nimport importlib.metadata\nfrom functools import lru_cache\nfrom typing import Optional\n\nfrom packaging import version\n\n# Handle version compatibility for flash_attn_supports_top_left_mask\n# This function was added in newer versions of transformers\ntry:\n    from transformers.modeling_flash_attention_utils import flash_attn_supports_top_left_mask\nexcept ImportError:\n    # For older versions of transformers that don't have this function\n    # Default to False as a safe fallback for older versions\n    def flash_attn_supports_top_left_mask():\n        \"\"\"Fallback implementation for older transformers versions.\n        Returns False to disable features that \n\n... [truncated 224 chars] ...\n\nrsion of the transformers library\n        transformers_version_str = importlib.metadata.version(\"transformers\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ModuleNotFoundError(\"The `transformers` package is not installed.\") from e\n\n    transformers_version = version.parse(transformers_version_str)\n\n    lower_bound_check = True\n    if min_version is not None:\n        lower_bound_check = version.parse(min_version) <= transformers_version\n\n    upper_bound_check = True\n    if max_version is not None:\n        upper_bound_check = transformers_version <= version.parse(max_version)\n\n    return lower_bound_check and upper_bound_check\n",
    "function_name": "verl__utils__transformers_compat",
    "file": "verl__utils__transformers_compat.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 57,
    "label_source": "synthetic_label"
  },
  {
    "query": "Draw a mermaid flowchart showing the execution paths within the `forward` method, detailing the decision nodes for training vs. inference, static vs. dynamic batching, and the specific attention kernel selected for each branch.",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "How does the `inference_context` abstraction unify KV cache management across the distinct execution paths (training, static inference, dynamic batching), and where does the responsibility for cache eviction or sliding window logic reside?",
    "code": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n# Copyright (c) 2024 Alibaba PAI Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.core.transformer.attention import *\n\nfrom .rope_utils import apply_rotary_pos_emb_absolute\n\n\nclass Qwen2_5VLSelfAttention(SelfAttention):\n    \"\"\"\n    Overrides the SelfAttention class, the difference is that qwen2_5_vl uses apply_rotary_pos_emb_absolute\n    instead of apply_rotary_pos_emb\n    \"\"\"\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Tensor,\n        key_value_states: Optional[Tensor] = None,\n        inference_context: Optional[BaseInferenceContext] = None,\n        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,\n        rotary_pos_cos: Optional[T\n\n... [truncated 7856 chars] ...\n\neze(0).unsqueeze(1)\n                core_attn_out = rearrange(core_attn_out, \"s b h d -> s b (h d)\")\n\n        if packed_seq_params is not None and packed_seq_params.qkv_format == \"thd\":\n            # reshape to same output shape as unpacked case\n            # (t, np, hn) -> (t, b=1, h=np*hn)\n            # t is the pack size = sum (sq_i)\n            # note that batch is a dummy dimension in the packed case\n            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.linear_proj(core_attn_out)\n\n        return output, bias\n",
    "function_name": "verl__models__mcore__qwen2_5_vl__attention",
    "file": "verl__models__mcore__qwen2_5_vl__attention.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 225
  },
  {
    "query": "RLHF value function loss computation",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImplement a multiprocess PPOCritic\n\"\"\"\n\nimport logging\nimport os\n\nimport torch\nimport torch.distributed\nfrom torch import nn, optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom verl import DataProto\nfrom verl.trainer.ppo import core_algos\nfrom verl.utils.attention_utils import index_first_axis, pad_input, rearrange, unpad_input\nfrom verl.utils.device import get_device_id, get_device_name\nfrom verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\nfrom verl.utils.profiler import GPUMemoryLogger\nfrom verl.utils.py_functional import append_to_dict\nfrom verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.u\n\n... [truncated 10318 chars] ...\n\n         {\n                            \"critic/vf_clipfrac\": vf_clipfrac.detach().item(),\n                            \"critic/vpred_mean\": masked_mean(vpreds, response_mask).detach().item(),\n                        }\n                    )\n\n                    metrics[\"critic/vf_loss\"] += vf_loss.detach().item() * loss_scale_factor\n                    append_to_dict(metrics, micro_batch_metrics)\n\n                grad_norm = self._optimizer_step()\n                mini_batch_metrics = {\"critic/grad_norm\": grad_norm.detach().item()}\n                append_to_dict(metrics, mini_batch_metrics)\n        self.critic_optimizer.zero_grad()\n        return metrics\n",
    "function_name": "verl__workers__critic__dp_critic",
    "file": "verl__workers__critic__dp_critic.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 263,
    "label_source": "synthetic_label"
  },
  {
    "query": "What is the architectural justification for using Apex's fused_rms_norm_affine instead of Megatron's native RMSNorm implementation or standard PyTorch layers, considering the operational overhead of maintaining Apex dependencies in a production RLHF cluster?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48
  },
  {
    "query": "Draw a mermaid diagram showing the class inheritance hierarchy between QKVParallelLinear, MergedColumnParallelLinear, and tensor_parallel.ColumnParallelLinear, including how input parameters are transformed into the final output_size argument passed to super().__init__.",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023 The vLLM team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py\n\n\nfrom megatron.core import tensor_parallel\n\n\nclass QKVParallelLinear(tensor_parallel.ColumnParallelLinear):\n    def __init__(\n        self,\n        input_size,\n        num_heads,\n        num_key_value_heads,\n        head_dim,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.q_output_size = num_heads * head_dim\n        self.kv_output_size = num_key_value_heads * head_dim\n        self.head_dim = head_dim\n        self.gather_\n\n... [truncated 543 chars] ...\n\n       gate_ouput_size,\n        up_output_size,\n        *,\n        bias=True,\n        gather_output=True,\n        skip_bias_add=False,\n        **kwargs,\n    ):\n        # Keep input parameters, and already restrict the head numbers\n        self.input_size = input_size\n        self.output_size = gate_ouput_size + up_output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        super().__init__(\n            input_size=self.input_size,\n            output_size=self.output_size,\n            bias=bias,\n            gather_output=gather_output,\n            skip_bias_add=skip_bias_add,\n            **kwargs,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_linear",
    "file": "verl__models__qwen2__megatron__layers__parallel_linear.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 79
  },
  {
    "query": "The load_module function conditionally adds the loaded module to sys.modules based on module_name. If module_name is None for a file path, the module isn't cached in sys.modules. What are the memory or performance implications of repeatedly loading large custom modules this way during long-running training jobs?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nUtilities to check if packages are available.\nWe assume package availability won't change during runtime.\n\"\"\"\n\nimport importlib\nimport importlib.util\nimport os\nimport warnings\nfrom functools import cache, wraps\nfrom typing import Optional\n\n\n@cache\ndef is_megatron_core_available():\n    try:\n        mcore_spec = importlib.util.find_spec(\"megatron.core\")\n    except ModuleNotFoundError:\n        mcore_spec = None\n    return mcore_spec is not None\n\n\n@cache\ndef is_vllm_available():\n    try:\n        vllm_spec = importlib.util.find_spec(\"vllm\")\n    except ModuleNotFoundError:\n        vllm_spec = None\n    return vllm_spec is not None\n\n\n@cache\ndef is_sglang_available():\n    try:\n        sglang_spec = importlib.util.find_spec(\"sglang\")\n  \n\n... [truncated 5860 chars] ...\n\nle_path, class_name = fqn.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module '{module_path}' for {description}: {e}\") from e\n    except AttributeError as e:\n        raise AttributeError(f\"Class '{class_name}' not found in module '{module_path}': {e}\") from e\n\n\n@deprecated(replacement=\"load_module(file_path); getattr(module, type_name)\")\ndef load_extern_type(file_path: str, type_name: str) -> type:\n    \"\"\"DEPRECATED. Directly use `load_extern_object` instead.\"\"\"\n    return load_extern_object(file_path, type_name)\n",
    "function_name": "verl__utils__import_utils",
    "file": "verl__utils__import_utils.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 236
  },
  {
    "query": "In the __init__ method, normalized_shape is assigned inside an if isinstance block but used outside; will this raise an UnboundLocalError if config.hidden_size is not a numbers.Integral, and what input types should be expected here?",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48
  },
  {
    "query": "RLHF training framework normalization module",
    "code": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numbers\n\nimport torch\nfrom apex.normalization.fused_layer_norm import fused_rms_norm_affine\nfrom megatron.core import ModelParallelConfig\nfrom torch import nn\nfrom transformers import Qwen2Config\n\nfrom verl.utils.megatron import sequence_parallel as sp_utils\n\n\nclass ParallelQwen2RMSNorm(nn.Module):\n    def __init__(self, config: Qwen2Config, megatron_config: ModelParallelConfig):\n        \"\"\"\n        Qwen2RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        if isinstance(config.hidden_size, numbers.Integral):\n            normalized_shape = (config.hidden_size,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n        self.variance_epsilon = config.rms_norm_eps\n\n        if megatron_config.sequence_parallel:\n            sp_utils.mark_parameter_as_sequence_parallel(self.weight)\n\n    def forward(self, hidden_states):\n        return fused_rms_norm_affine(\n            input=hidden_states,\n            weight=self.weight,\n            normalized_shape=self.normalized_shape,\n            eps=self.variance_epsilon,\n            memory_efficient=True,\n        )\n",
    "function_name": "verl__models__qwen2__megatron__layers__parallel_rmsnorm",
    "file": "verl__models__qwen2__megatron__layers__parallel_rmsnorm.py",
    "repo": "verl",
    "language": "Python",
    "num_lines": 48,
    "label_source": "synthetic_label"
  }
]