{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Track B \u2013 SFT Fine-tuning Demo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83e\udde0 Track B \u2013 SFT Fine-tuning Demo\n\nFine-tune `Qwen/Qwen2.5-Coder-1.5B` on a synthetic Python coding dataset and measure improvement.\n\n---\n\n## \ud83d\udce6 Dataset: [`archit11/track_b_sft`](https://huggingface.co/datasets/archit11/track_b_sft)\n\n| Field | Detail |\n|-------|--------|\n| **Source** | [verl](https://github.com/volcengine/verl) Python library (AST-extracted functions) |\n| **Train** | 514 examples |\n| **Test** | 46 examples |\n| **Gold responses** | Generated by `Qwen3-Coder-30B-A3B-Instruct` via vLLM |\n| **License** | Apache 2.0 |\n\n### Task Categories\n\n| Category | Description |\n|----------|-------------|\n| `docstring` | Write a Google-style docstring for a real function |\n| `explain` | Explain what a function does and how |\n| `bugfix` | Identify and fix a deterministically injected bug |\n| `complete` | Complete a function body given its signature + docstring |\n| `unit_test` | Write pytest tests with assertions |\n\n---\n\n## \ud83e\udd16 Model: [`archit11/track_b_sft_merged`](https://huggingface.co/archit11/track_b_sft_merged)\n\n| Field | Detail |\n|-------|--------|\n| **Base** | `Qwen/Qwen2.5-Coder-1.5B` |\n| **Method** | LoRA (r=16, alpha=32) \u2192 merged |\n| **Epochs** | 3 |\n| **LR** | 2e-4, cosine schedule |\n| **Hardware** | T4 GPU, fp16 |\n| **Training time** | ~56 seconds |\n\n---\n\n## \ud83d\udcca Results (Reproduced Below)\n\n| Metric | Baseline | Post-SFT | \u0394 |\n|--------|----------|----------|---|\n| **pass@1** | 0.565 | **0.804** | **+0.239 \u2191** |\n| **pass@3** | 0.783 | 0.848 | +0.065 \u2191 |\n| **style score** | 0.874 | 0.848 | \u22120.026 |\n\n| Category | Baseline | Post-SFT | \u0394 |\n|----------|----------|----------|---|\n| bugfix | 0.17 | \u2014 | \u2014 |\n| complete | 0.45 | \u2014 | \u2014 |\n| docstring | 0.40 | \u2014 | \u2014 |\n| explain | 1.00 | 1.00 | 0.00 |\n| unit_test | 1.00 | \u2014 | \u2014 |\n\n> \u26a1 **Make sure Runtime \u2192 Change runtime type \u2192 T4 GPU is selected before running.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 \u2013 Install dependencies\n",
    "!pip install -q transformers==5.2.0 peft==0.18.1 trl==0.28.0 datasets accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 \u2013 Imports & config\n",
    "import ast, json, os, time\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import inspect\n",
    "\n",
    "BASE_MODEL   = \"Qwen/Qwen2.5-Coder-1.5B\"\n",
    "DATASET_REPO = \"archit11/track_b_sft\"\n",
    "OUTPUT_DIR   = \"/content/track_b_sft\"\n",
    "EPOCHS       = 3\n",
    "BATCH_SIZE   = 4\n",
    "GRAD_ACCUM   = 4\n",
    "LR           = 2e-4\n",
    "MAX_LEN      = 1024\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"\u2713 Device: {DEVICE}\")\n",
    "print(f\"\u2713 Base model: {BASE_MODEL}\")\n",
    "print(f\"\u2713 Dataset: {DATASET_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 \u2013 Load dataset from Hugging Face\n",
    "print(f\"Loading {DATASET_REPO} from Hugging Face...\")\n",
    "hf_ds = load_dataset(DATASET_REPO)\n",
    "train_data = list(hf_ds[\"train\"])\n",
    "test_data  = list(hf_ds[\"test\"])\n",
    "print(f\"\u2713 Train: {len(train_data)} examples\")\n",
    "print(f\"\u2713 Test:  {len(test_data)} examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "ex = train_data[0]\n",
    "print(f\"  Category:    {ex['category']}\")\n",
    "print(f\"  Instruction: {ex['instruction'][:120]}...\")\n",
    "print(f\"  Response:    {ex['response'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 \u2013 Evaluation helpers\n",
    "def load_model_and_tokenizer(model_path, is_lora=False, base=BASE_MODEL):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    tok.pad_token = tok.eos_token\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        base if is_lora else model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    if is_lora:\n",
    "        m = PeftModel.from_pretrained(m, model_path).merge_and_unload()\n",
    "    m.eval()\n",
    "    return m, tok\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new=256):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new,\n",
    "                             do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def check_pass(response, category):\n",
    "    r = response.strip()\n",
    "    if not r or len(r) < 10:\n",
    "        return False\n",
    "    if category == \"docstring\":\n",
    "        return '\"\"\"' in r or \"'''\" in r\n",
    "    if category == \"complete\":\n",
    "        try: ast.parse(r); return True\n",
    "        except: return \"def \" in r or \"return\" in r\n",
    "    if category == \"bugfix\":\n",
    "        return any(w in r.lower() for w in [\"fix\", \"bug\", \"error\", \"change\", \"replace\", \"correct\"])\n",
    "    if category == \"explain\":\n",
    "        return len(r.split()) >= 20\n",
    "    if category == \"unit_test\":\n",
    "        return \"def test_\" in r and \"assert\" in r\n",
    "    return True\n",
    "\n",
    "def evaluate(model, tokenizer, test_data, tag):\n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\"  Evaluating: {tag}  ({len(test_data)} examples)\")\n",
    "    print(f\"{'='*55}\")\n",
    "    results, t0 = [], time.time()\n",
    "    for i, ex in enumerate(test_data):\n",
    "        prompt = (f\"<|im_start|>user\\n{ex['instruction']}<|im_end|>\\n\"\n",
    "                  f\"<|im_start|>assistant\\n\")\n",
    "        resp   = generate(model, tokenizer, prompt)\n",
    "        passed = check_pass(resp, ex[\"category\"])\n",
    "        results.append({\"category\": ex[\"category\"], \"pass\": passed})\n",
    "        print(f\"  [{i+1:2d}/{len(test_data)}] {ex['category']:12s} {'\u2713' if passed else '\u2717'}\")\n",
    "\n",
    "    total  = len(results)\n",
    "    passed = sum(r[\"pass\"] for r in results)\n",
    "    by_cat = {}\n",
    "    for r in results:\n",
    "        c = r[\"category\"]\n",
    "        by_cat.setdefault(c, {\"n\": 0, \"p\": 0})\n",
    "        by_cat[c][\"n\"] += 1\n",
    "        by_cat[c][\"p\"] += r[\"pass\"]\n",
    "\n",
    "    print(f\"\\n  pass@1: {passed/total:.3f}  ({passed}/{total})\")\n",
    "    print(f\"  Wall time: {time.time()-t0:.1f}s\")\n",
    "    print(f\"\\n  Per-category:\")\n",
    "    for cat, v in sorted(by_cat.items()):\n",
    "        bar = '\u2588' * v['p'] + '\u2591' * (v['n'] - v['p'])\n",
    "        print(f\"    {cat:12s}  {bar}  {v['p']}/{v['n']}\")\n",
    "    return {\"tag\": tag, \"pass@1\": passed/total, \"by_category\": by_cat}\n",
    "\n",
    "print(\"\u2713 Helpers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 \u2013 Baseline evaluation\n",
    "print(\"Loading base model for baseline evaluation...\")\n",
    "model, tokenizer = load_model_and_tokenizer(BASE_MODEL)\n",
    "baseline_results = evaluate(model, tokenizer, test_data, \"baseline\")\n",
    "\n",
    "# Free GPU memory before training\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\u2713 GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 \u2013 Fine-tune with LoRA\n",
    "print(\"Loading model for fine-tuning...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Format dataset as ChatML text\n",
    "train_dataset = Dataset.from_list([\n",
    "    {\"text\": (f\"<|im_start|>user\\n{d['instruction']}<|im_end|>\\n\"\n",
    "               f\"<|im_start|>assistant\\n{d['response']}<|im_end|>\")}\n",
    "    for d in train_data\n",
    "])\n",
    "print(f\"\u2713 {len(train_dataset)} training examples formatted\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Handle SFTConfig API differences across trl versions\n",
    "_sft_params = set(inspect.signature(SFTConfig.__init__).parameters)\n",
    "_sft_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "if \"max_length\" in _sft_params:\n",
    "    _sft_kwargs[\"max_length\"] = MAX_LEN\n",
    "elif \"max_seq_length\" in _sft_params:\n",
    "    _sft_kwargs[\"max_seq_length\"] = MAX_LEN\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=SFTConfig(**_sft_kwargs),\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "t0 = time.time()\n",
    "trainer.train()\n",
    "print(f\"\\n\u2713 Training complete in {time.time()-t0:.1f}s\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"\u2713 Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 \u2013 Post-SFT evaluation\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading fine-tuned model (merging LoRA)...\")\n",
    "ft_model, ft_tokenizer = load_model_and_tokenizer(OUTPUT_DIR, is_lora=True)\n",
    "postsft_results = evaluate(ft_model, ft_tokenizer, test_data, \"post_sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 \u2013 Final comparison\n",
    "b = baseline_results[\"pass@1\"]\n",
    "a = postsft_results[\"pass@1\"]\n",
    "delta = a - b\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"  FINAL COMPARISON\")\n",
    "print(\"=\"*55)\n",
    "print(f\"  Baseline  pass@1 : {b:.3f}\")\n",
    "print(f\"  Post-SFT  pass@1 : {a:.3f}\")\n",
    "print(f\"  Delta            : {delta:+.3f}  {'\u2713 IMPROVED' if delta > 0 else '\u2717 REGRESSED'}\")\n",
    "print()\n",
    "print(f\"  {'Category':<12}  {'Baseline':>8}  {'Post-SFT':>8}  {'Delta':>7}\")\n",
    "print(f\"  {'-'*12}  {'-'*8}  {'-'*8}  {'-'*7}\")\n",
    "all_cats = set(baseline_results[\"by_category\"]) | set(postsft_results[\"by_category\"])\n",
    "for cat in sorted(all_cats):\n",
    "    bv = baseline_results[\"by_category\"].get(cat, {\"p\": 0, \"n\": 1})\n",
    "    av = postsft_results[\"by_category\"].get(cat, {\"p\": 0, \"n\": 1})\n",
    "    bd, ad = bv[\"p\"]/bv[\"n\"], av[\"p\"]/av[\"n\"]\n",
    "    print(f\"  {cat:<12}  {bd:>8.2f}  {ad:>8.2f}  {ad-bd:>+7.2f}\")\n",
    "print(\"=\"*55)"
   ]
  }
 ]
}