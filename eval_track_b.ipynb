{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-header",
   "metadata": {},
   "source": [
    "# Track B – SFT Output Comparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/archit-spec/assesment-jp/blob/main/eval_track_b.ipynb)\n",
    "\n",
    "Side-by-side output comparison of **baseline** (`Qwen/Qwen2.5-Coder-1.5B`) vs **fine-tuned** (`archit11/track_b_sft_merged`) on 5 test samples.\n",
    "\n",
    "**Dataset:** [`archit11/track_b_sft`](https://huggingface.co/datasets/archit11/track_b_sft)\n",
    "\n",
    "### Install dependencies\n",
    "```bash\n",
    "pip install transformers datasets torch accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-eval",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B\"\n",
    "FT_MODEL_ID   = \"archit11/track_b_sft_merged\"\n",
    "DATASET_ID    = \"archit11/track_b_sft\"\n",
    "NUM_SAMPLES   = 5\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load 5 test samples\n",
    "ds = load_dataset(DATASET_ID, split=\"test\")\n",
    "samples = ds.select(range(min(NUM_SAMPLES, len(ds))))\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "\n",
    "def load_model(model_id):\n",
    "    print(f\"Loading {model_id}...\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, trust_remote_code=True\n",
    "    ).to(device).eval()\n",
    "    return model, tok\n",
    "\n",
    "def generate(model, tok, instruction):\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "    try:\n",
    "        text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except:\n",
    "        text = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS,\n",
    "                             do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "    return tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "# Run base model\n",
    "base_model, tok = load_model(BASE_MODEL_ID)\n",
    "base_outputs = [generate(base_model, tok, row[\"instruction\"]) for row in samples]\n",
    "del base_model\n",
    "import gc; gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run fine-tuned model\n",
    "ft_model, tok = load_model(FT_MODEL_ID)\n",
    "ft_outputs = [generate(ft_model, tok, row[\"instruction\"]) for row in samples]\n",
    "del ft_model\n",
    "gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Print comparison\n",
    "SEP = \"=\" * 70\n",
    "for i, row in enumerate(samples):\n",
    "    print(f\"\\n{SEP}\")\n",
    "    print(f\"SAMPLE {i+1} | category: {row.get('category', 'n/a')}\")\n",
    "    print(f\"{SEP}\")\n",
    "    print(f\"INSTRUCTION:\\n{row['instruction'][:300]}\")\n",
    "    print(f\"\\n--- BASE MODEL ---\")\n",
    "    print(base_outputs[i][:400])\n",
    "    print(f\"\\n--- FINE-TUNED ---\")\n",
    "    print(ft_outputs[i][:400])\n",
    "print(f\"\\n{SEP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-results",
   "metadata": {},
   "source": [
    "### Known Results (pass@1 / pass@3)\n",
    "\n",
    "| Metric | Baseline | Fine-Tuned | Δ |\n",
    "|--------|----------|------------|---|\n",
    "| pass@1 | 0.565 | **0.804** | +23.9% ↑ |\n",
    "| pass@3 | 0.783 | **0.848** | +6.5% ↑ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3z138p4zgv",
   "source": "## Style Analysis – PEP8 & Docstring Quality",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "aqeuxdjw3pl",
   "source": "import ast, re, subprocess, sys, tempfile, os\n\ndef check_pep8(code: str) -> dict:\n    \"\"\"Run pycodestyle on code string, return violation count and details.\"\"\"\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(code); fname = f.name\n    try:\n        r = subprocess.run(\n            [sys.executable, \"-m\", \"pycodestyle\", \"--max-line-length=100\", fname],\n            capture_output=True, text=True\n        )\n        lines = [l for l in r.stdout.strip().splitlines() if l]\n        return {\"violations\": len(lines), \"details\": lines[:5]}\n    except Exception as e:\n        return {\"violations\": -1, \"details\": [str(e)]}\n    finally:\n        os.unlink(fname)\n\ndef check_docstring(text: str) -> dict:\n    \"\"\"Check for Google/NumPy style docstring markers.\"\"\"\n    has_triple  = '\"\"\"' in text or \"'''\" in text\n    has_summary = has_triple and len(text.strip()) > 40\n    has_args    = bool(re.search(r\"Args:|Parameters:|:param \", text))\n    has_returns = bool(re.search(r\"Returns:|:returns:|Return:\", text))\n    has_example = bool(re.search(r\"Example[s]?:|>>>\" , text))\n    score = sum([has_triple, has_summary, has_args, has_returns, has_example]) / 5\n    return {\n        \"score\": round(score, 2),\n        \"triple_quotes\": has_triple,\n        \"summary\": has_summary,\n        \"args\": has_args,\n        \"returns\": has_returns,\n        \"examples\": has_example,\n    }\n\ndef extract_code(text: str) -> str:\n    blocks = re.findall(r\"```(?:python)?\\s*\\n(.*?)```\", text, re.DOTALL)\n    return blocks[0].strip() if blocks else text.strip()\n\nSEP = \"=\" * 70\nprint(f\"{'Sample':<8} {'Model':<12} {'PEP8 violations':>16} {'Docstring score':>15}\")\nprint(\"-\" * 55)\n\nfor i, row in enumerate(samples):\n    for label, output in [(\"Base\", base_outputs[i]), (\"Fine-Tuned\", ft_outputs[i])]:\n        code = extract_code(output)\n        pep8   = check_pep8(code)\n        docstr = check_docstring(output)\n        viols  = pep8[\"violations\"] if pep8[\"violations\"] >= 0 else \"n/a\"\n        print(f\"  [{i+1}]   {label:<12} {str(viols):>16} {docstr['score']:>15.2f}\")\n        if pep8[\"details\"]:\n            for d in pep8[\"details\"][:2]:\n                print(f\"           └ {d.split(':',2)[-1].strip()[:60]}\")\n\nprint(f\"\\n{SEP}\")\nprint(\"DOCSTRING DETAIL (Fine-Tuned only)\")\nprint(SEP)\nfor i, row in enumerate(samples):\n    d = check_docstring(ft_outputs[i])\n    checks = {k: (\"✓\" if v else \"✗\") for k, v in d.items() if k != \"score\"}\n    print(f\"  [{i+1}] score={d['score']:.2f}  \" + \"  \".join(f\"{k}:{v}\" for k,v in checks.items()))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}