{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4",
            "name": "Track A \u2013 Extended Pre-Training Demo"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# \ud83e\udde0 Track A \u2013 Extended Pre-Training Demo\n\nContinue pre-training `Qwen/Qwen2.5-Coder-3B` on a curated Rust code corpus extracted from the [juspay/hyperswitch](https://github.com/juspay/hyperswitch) repository, then measure perplexity improvement.\n\n---\n\n## \ud83d\udce6 Dataset: [`archit11/hyperswitch-code-corpus-track-a`](https://huggingface.co/datasets/archit11/hyperswitch-code-corpus-track-a)\n\n| Field | Detail |\n|-------|--------|\n| **Source** | `juspay/hyperswitch` \u2013 `crates/` Rust files only |\n| **Total files** | 300 (top-ranked by quality score) |\n| **Train** | 270 files |\n| **Validation** | 30 files |\n| **File format** | `file_name` + `text` (full file contents) |\n| **License** | Apache 2.0 |\n\n### Data Card Summary\n\n| Filter | Detail |\n|--------|--------|\n| **Path filter** | `crates/` only, excludes `tests/`, `docs/`, `examples/`, `migrations/` |\n| **Line count** | 25 \u2013 4000 lines per file |\n| **Quality filter** | Structurally rich files (functions + types \u2265 2) |\n| **Ranking** | Top 300 by quality score from 1,526 candidates |\n| **Chunking** | Fixed-size non-overlapping windows; `// FILE:` header per file |\n| **Curriculum** | Sequence lengths 768 \u2192 1024 \u2192 1536 |\n\n---\n\n## \ud83e\udd16 Model: [`archit11/qwen2.5-coder-3b-hyperswitch-track-a-merged`](https://huggingface.co/archit11/qwen2.5-coder-3b-hyperswitch-track-a-merged)\n\n| Field | Detail |\n|-------|--------|\n| **Base** | `Qwen/Qwen2.5-Coder-3B` |\n| **Method** | LoRA continued pre-training \u2192 merged |\n| **LR** | 1e-3, cosine schedule |\n| **Batch size** | 1 (gradient accumulation) |\n| **Curriculum** | 768 \u2192 1024 \u2192 1536 token chunks |\n| **Hardware** | T4 GPU, fp16 |\n\n---\n\n## \ud83d\udcca Results (Reproduced Below)\n\n| Metric | Baseline | Post-Training | \u0394 |\n|--------|----------|---------------|---|\n| **Perplexity** | 2.2832 | **1.5429** | **\u221232.42%** |\n\n> \u26a1 **Make sure Runtime \u2192 Change runtime type \u2192 T4 GPU is selected before running.**\n\n> \ud83d\udccc Lower perplexity = better: the model assigns higher probability to real Hyperswitch code."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 \u2013 Install dependencies\n",
                "!pip install -q transformers==5.2.0 peft==0.18.1 datasets accelerate huggingface_hub\n",
                "print(\"\u2713 Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 \u2013 Imports & config\n",
                "import math, time, json, random\n",
                "import torch\n",
                "import numpy as np\n",
                "from datasets import load_dataset, Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForCausalLM,\n",
                "    TrainingArguments, Trainer,\n",
                "    DataCollatorForLanguageModeling,\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, PeftModel\n",
                "\n",
                "BASE_MODEL    = \"Qwen/Qwen2.5-Coder-3B\"\n",
                "MERGED_HF     = \"archit11/qwen2.5-coder-3b-hyperswitch-track-a-merged\"\n",
                "DATASET_REPO  = \"archit11/hyperswitch-code-corpus-track-a\"\n",
                "OUTPUT_DIR    = \"/content/track_a_lora\"\n",
                "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Training hyperparameters (T4-safe)\n",
                "BLOCK_SIZES   = [768, 1024, 1536]   # curriculum sequence lengths\n",
                "EPOCHS_PER    = 1                   # 1 epoch per curriculum stage\n",
                "BATCH_SIZE    = 1\n",
                "GRAD_ACCUM    = 8\n",
                "LR            = 1e-3\n",
                "MAX_EVAL_CHUNKS = 160               # cap validation chunks for speed\n",
                "\n",
                "print(f\"\u2713 Device : {DEVICE}\")\n",
                "print(f\"\u2713 Base   : {BASE_MODEL}\")\n",
                "print(f\"\u2713 Dataset: {DATASET_REPO}\")\n",
                "print(f\"\u2713 Curriculum: {BLOCK_SIZES}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3 \u2013 Load dataset & tokenizer\n",
                "print(f\"Loading {DATASET_REPO} ...\")\n",
                "ds = load_dataset(DATASET_REPO)\n",
                "train_files = list(ds[\"train\"])\n",
                "val_files   = list(ds[\"validation\"]) if \"validation\" in ds else list(ds[\"train\"])[-30:]\n",
                "\n",
                "print(f\"\u2713 Train files : {len(train_files)}\")\n",
                "print(f\"\u2713 Val files   : {len(val_files)}\")\n",
                "print(f\"\\nSample file: {train_files[0]['file_name']}\")\n",
                "print(f\"  Length: {len(train_files[0]['text'])} chars\")\n",
                "\n",
                "print(f\"\\nLoading tokenizer: {BASE_MODEL} ...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\n",
                "    BASE_MODEL, trust_remote_code=True, fix_mistral_regex=True\n",
                ")\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "print(f\"\u2713 Tokenizer loaded (vocab size: {tokenizer.vocab_size:,})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4 \u2013 Chunking & perplexity helpers\n",
                "\n",
                "def build_chunks(file_list, block_size, max_chunks=None):\n",
                "    \"\"\"\n",
                "    Concatenate files with // FILE: headers, tokenize, and split into\n",
                "    fixed-size non-overlapping chunks of `block_size` tokens.\n",
                "    \"\"\"\n",
                "    all_ids = []\n",
                "    for item in file_list:\n",
                "        header = f\"// FILE: {item['file_name']}\\n\"\n",
                "        text   = header + item[\"text\"] + tokenizer.eos_token\n",
                "        ids    = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
                "        all_ids.extend(ids)\n",
                "\n",
                "    # Split into fixed-size chunks\n",
                "    chunks = [\n",
                "        all_ids[i : i + block_size]\n",
                "        for i in range(0, len(all_ids) - block_size + 1, block_size)\n",
                "    ]\n",
                "\n",
                "    if max_chunks and len(chunks) > max_chunks:\n",
                "        random.shuffle(chunks)\n",
                "        chunks = chunks[:max_chunks]\n",
                "\n",
                "    return chunks\n",
                "\n",
                "\n",
                "def chunks_to_dataset(chunks):\n",
                "    \"\"\"Convert list of token-id chunks to a HF Dataset.\"\"\"\n",
                "    return Dataset.from_dict({\n",
                "        \"input_ids\":      chunks,\n",
                "        \"attention_mask\": [[1] * len(c) for c in chunks],\n",
                "        \"labels\":         chunks,\n",
                "    })\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def compute_perplexity(model, chunks, batch_size=4):\n",
                "    \"\"\"\n",
                "    Compute perplexity as exp(mean CE loss) over all chunks.\n",
                "    PPL = exp( sum(loss * n_tokens) / total_tokens )\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0.0\n",
                "    total_toks = 0\n",
                "\n",
                "    for i in range(0, len(chunks), batch_size):\n",
                "        batch = chunks[i : i + batch_size]\n",
                "        ids   = torch.tensor(batch, dtype=torch.long).to(DEVICE)\n",
                "        out   = model(input_ids=ids, labels=ids)\n",
                "        n_tok = ids.numel()\n",
                "        total_loss += out.loss.item() * n_tok\n",
                "        total_toks += n_tok\n",
                "\n",
                "    return math.exp(total_loss / total_toks)\n",
                "\n",
                "\n",
                "print(\"\u2713 Helpers defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5 \u2013 Baseline perplexity (Qwen2.5-Coder-3B, no fine-tuning)\n",
                "print(f\"Loading base model: {BASE_MODEL} ...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    torch_dtype=torch.float16,\n",
                "    trust_remote_code=True,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "model.eval()\n",
                "\n",
                "# Build validation chunks at the largest block size for a stable PPL estimate\n",
                "print(\"Building validation chunks (block_size=1536) ...\")\n",
                "val_chunks = build_chunks(val_files, block_size=1536, max_chunks=MAX_EVAL_CHUNKS)\n",
                "print(f\"\u2713 {len(val_chunks)} validation chunks\")\n",
                "\n",
                "print(\"Computing baseline perplexity ...\")\n",
                "t0 = time.time()\n",
                "baseline_ppl = compute_perplexity(model, val_chunks)\n",
                "print(f\"\u2713 Baseline perplexity : {baseline_ppl:.4f}  ({time.time()-t0:.1f}s)\")\n",
                "\n",
                "# Free GPU memory before training\n",
                "del model\n",
                "torch.cuda.empty_cache()\n",
                "print(\"\u2713 GPU memory freed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6 \u2013 LoRA continued pre-training with sequence-length curriculum\n",
                "#\n",
                "# Curriculum: train on progressively longer chunks (768 -> 1024 -> 1536)\n",
                "# This matches how the uploaded model was trained.\n",
                "# Loss: standard next-token prediction (causal LM), no masking needed.\n",
                "\n",
                "print(f\"Loading model for training: {BASE_MODEL} ...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    torch_dtype=torch.float16,\n",
                "    trust_remote_code=True,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "# LoRA config\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.05,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()\n",
                "\n",
                "curriculum_ppl = {}\n",
                "\n",
                "for block_size in BLOCK_SIZES:\n",
                "    print(f\"\\n{'='*55}\")\n",
                "    print(f\"  Curriculum stage: block_size={block_size}\")\n",
                "    print(f\"{'='*55}\")\n",
                "\n",
                "    train_chunks = build_chunks(train_files, block_size=block_size)\n",
                "    print(f\"  Train chunks: {len(train_chunks)}\")\n",
                "\n",
                "    train_dataset = chunks_to_dataset(train_chunks)\n",
                "\n",
                "    stage_output = f\"{OUTPUT_DIR}/stage_{block_size}\"\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=stage_output,\n",
                "        num_train_epochs=EPOCHS_PER,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRAD_ACCUM,\n",
                "        learning_rate=LR,\n",
                "        lr_scheduler_type=\"cosine\",\n",
                "        warmup_ratio=0.05,\n",
                "        fp16=True,\n",
                "        logging_steps=20,\n",
                "        save_strategy=\"no\",\n",
                "        report_to=\"none\",\n",
                "        dataloader_num_workers=0,\n",
                "        remove_unused_columns=False,\n",
                "    )\n",
                "\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=train_dataset,\n",
                "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
                "    )\n",
                "\n",
                "    t0 = time.time()\n",
                "    trainer.train()\n",
                "    elapsed = time.time() - t0\n",
                "\n",
                "    # Eval PPL at this stage\n",
                "    stage_chunks = build_chunks(val_files, block_size=block_size, max_chunks=MAX_EVAL_CHUNKS)\n",
                "    stage_ppl    = compute_perplexity(model, stage_chunks)\n",
                "    curriculum_ppl[block_size] = stage_ppl\n",
                "    print(f\"  \u2713 Stage PPL: {stage_ppl:.4f}  (trained in {elapsed:.1f}s)\")\n",
                "\n",
                "# Save LoRA adapter\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(f\"\\n\u2713 LoRA adapter saved to {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7 \u2013 Post-training perplexity (final evaluation)\n",
                "print(\"Computing post-training perplexity on validation set ...\")\n",
                "final_chunks = build_chunks(val_files, block_size=1536, max_chunks=MAX_EVAL_CHUNKS)\n",
                "posttrain_ppl = compute_perplexity(model, final_chunks)\n",
                "print(f\"\u2713 Post-training perplexity: {posttrain_ppl:.4f}\")\n",
                "\n",
                "# Also evaluate the uploaded merged model for reference\n",
                "del model\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(f\"\\nLoading uploaded HF merged model: {MERGED_HF} ...\")\n",
                "try:\n",
                "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
                "        MERGED_HF, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
                "    )\n",
                "    hf_ppl = compute_perplexity(hf_model, final_chunks)\n",
                "    print(f\"\u2713 HF merged model perplexity: {hf_ppl:.4f}\")\n",
                "    del hf_model\n",
                "    torch.cuda.empty_cache()\n",
                "except Exception as e:\n",
                "    print(f\"  (Could not load HF model: {e})\")\n",
                "    hf_ppl = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8 \u2013 Final comparison & curriculum summary\n",
                "improvement = (baseline_ppl - posttrain_ppl) / baseline_ppl * 100\n",
                "\n",
                "print(\"\\n\" + \"=\"*55)\n",
                "print(\"  FINAL COMPARISON\")\n",
                "print(\"=\"*55)\n",
                "print(f\"  {'Metric':<25}  {'Value':>10}\")\n",
                "print(f\"  {'-'*25}  {'-'*10}\")\n",
                "print(f\"  {'Baseline PPL':<25}  {baseline_ppl:>10.4f}\")\n",
                "print(f\"  {'Post-training PPL':<25}  {posttrain_ppl:>10.4f}\")\n",
                "print(f\"  {'Improvement':<25}  {improvement:>9.2f}%\")\n",
                "if hf_ppl:\n",
                "    print(f\"  {'HF merged model PPL':<25}  {hf_ppl:>10.4f}\")\n",
                "print(\"=\"*55)\n",
                "\n",
                "print(\"\\n  Curriculum PPL progression:\")\n",
                "print(f\"  {'Block size':<12}  {'Val PPL':>8}\")\n",
                "print(f\"  {'-'*12}  {'-'*8}\")\n",
                "print(f\"  {'baseline':<12}  {baseline_ppl:>8.4f}\")\n",
                "for bs, ppl in curriculum_ppl.items():\n",
                "    print(f\"  {bs:<12}  {ppl:>8.4f}\")\n",
                "\n",
                "# Save metrics JSON\n",
                "metrics = {\n",
                "    \"baseline_ppl\":    baseline_ppl,\n",
                "    \"posttrain_ppl\":   posttrain_ppl,\n",
                "    \"improvement_pct\": improvement,\n",
                "    \"curriculum_ppl\":  {str(k): v for k, v in curriculum_ppl.items()},\n",
                "}\n",
                "if hf_ppl:\n",
                "    metrics[\"hf_merged_ppl\"] = hf_ppl\n",
                "\n",
                "with open(\"/content/track_a_metrics.json\", \"w\") as f:\n",
                "    json.dump(metrics, f, indent=2)\n",
                "print(\"\\n\u2713 Metrics saved to /content/track_a_metrics.json\")\n",
                "print(\"\u2713 Track A complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9 \u2013 [Optional] Merge LoRA adapter and push to Hugging Face\n",
                "# Uncomment to merge and push the full model\n",
                "\n",
                "# from huggingface_hub import login\n",
                "# login(token=\"hf_YOUR_TOKEN_HERE\")\n",
                "#\n",
                "# print(\"Merging LoRA adapter into base model ...\")\n",
                "# base = AutoModelForCausalLM.from_pretrained(\n",
                "#     BASE_MODEL, torch_dtype=torch.float16, trust_remote_code=True\n",
                "# )\n",
                "# merged = PeftModel.from_pretrained(base, OUTPUT_DIR).merge_and_unload()\n",
                "# merged.push_to_hub(\"YOUR_HF_USERNAME/track_a_merged\")\n",
                "# tokenizer.push_to_hub(\"YOUR_HF_USERNAME/track_a_merged\")\n",
                "# print(\"\u2713 Merged model pushed to Hugging Face Hub\")\n",
                "\n",
                "print(\"Skipping merge/upload (uncomment above to push to HF Hub)\")"
            ]
        }
    ]
}