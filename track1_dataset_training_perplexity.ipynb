{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Track 1: Dataset + Model Training + Perplexity\n",
        "\n",
        "Self-contained notebook for Track 1:\n",
        "\n",
        "1. Load dataset from Hugging Face\n",
        "2. Build token chunks\n",
        "3. Train LoRA model (optional)\n",
        "4. Load model from HF (or local output) and show perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "os.environ['UV_CACHE_DIR'] = '/tmp/uv-cache'\n",
        "ROOT = Path('.').resolve()\n",
        "print('ROOT =', ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data\n",
        "HF_DATASET_REPO = 'archit11/hyperswitch-code-corpus-track-a'\n",
        "\n",
        "# Base model for training\n",
        "BASE_MODEL_ID = 'Qwen/Qwen2.5-Coder-3B'\n",
        "\n",
        "# Model to load at the end for final perplexity display.\n",
        "# Use merged full model repo for direct loading.\n",
        "HF_FINAL_MODEL_REPO = 'archit11/qwen2.5-coder-3b-hyperswitch-track-a-merged'\n",
        "\n",
        "# If True, run LoRA training in this notebook.\n",
        "RUN_TRAINING = False\n",
        "\n",
        "# Training hyperparams\n",
        "SEED = 42\n",
        "SEQUENCE_CURRICULUM = [768, 1024, 1536]\n",
        "EVAL_BLOCK_SIZE = 1536\n",
        "TOTAL_EPOCHS = 3.0\n",
        "BATCH_SIZE = 1\n",
        "EVAL_BATCH_SIZE = 1\n",
        "GRAD_ACCUM_STEPS = 8\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_GRAD_NORM = 0.5\n",
        "MAX_TRAIN_CHUNKS = 900\n",
        "MAX_VAL_CHUNKS = 160\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = Path('results/track1_hf_self_contained')\n",
        "MODEL_DIR = OUTPUT_DIR / 'model'\n",
        "METRICS_FILE = OUTPUT_DIR / 'track1_metrics.json'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = torch.float16 if device == 'cuda' else torch.float32\n",
        "\n",
        "print('device =', device)\n",
        "print('run_training =', RUN_TRAINING)\n",
        "print('output_dir =', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load Dataset from HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_ds = load_dataset(HF_DATASET_REPO)\n",
        "print(hf_ds)\n",
        "for split_name in hf_ds.keys():\n",
        "    print(split_name, len(hf_ds[split_name]))\n",
        "\n",
        "assert 'train' in hf_ds, 'Dataset must contain train split'\n",
        "if 'validation' in hf_ds:\n",
        "    train_split = hf_ds['train']\n",
        "    val_split = hf_ds['validation']\n",
        "else:\n",
        "    split = hf_ds['train'].train_test_split(test_size=0.1, seed=SEED)\n",
        "    train_split = split['train']\n",
        "    val_split = split['test']\n",
        "\n",
        "print('train files =', len(train_split))\n",
        "print('val files   =', len(val_split))\n",
        "print('columns     =', train_split.column_names)\n",
        "print('sample file =', train_split[0]['file_name'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Helpers (Chunking + Perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def file_to_text(row):\n",
        "    return f\"// FILE: {row['file_name']}\\n{row['text']}\"\n",
        "\n",
        "def build_chunk_dataset(split_ds, tokenizer, block_size):\n",
        "    chunks = []\n",
        "    for row in split_ds:\n",
        "        token_ids = tokenizer(\n",
        "            file_to_text(row),\n",
        "            add_special_tokens=False,\n",
        "            return_attention_mask=False,\n",
        "        )['input_ids']\n",
        "        for i in range(0, len(token_ids) - block_size + 1, block_size):\n",
        "            chunks.append({'input_ids': token_ids[i:i + block_size]})\n",
        "    return Dataset.from_list(chunks)\n",
        "\n",
        "def maybe_trim(ds, limit, seed):\n",
        "    if limit <= 0 or len(ds) <= limit:\n",
        "        return ds\n",
        "    idx = list(range(len(ds)))\n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(idx)\n",
        "    idx = sorted(idx[:limit])\n",
        "    return ds.select(idx)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_ppl(model, ds, batch_size=1):\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    for i in range(0, len(ds), batch_size):\n",
        "        b = ds[i:i + batch_size]\n",
        "        input_ids = torch.tensor(b['input_ids'], device=dev)\n",
        "        out = model(input_ids=input_ids, labels=input_ids)\n",
        "        n = input_ids.numel()\n",
        "        total_loss += out.loss.item() * n\n",
        "        total_tokens += n\n",
        "    return math.exp(total_loss / total_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Baseline Perplexity (Base Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=dtype,\n",
        ")\n",
        "base_model.to(device)\n",
        "\n",
        "val_ds_eval = build_chunk_dataset(val_split, base_tokenizer, EVAL_BLOCK_SIZE)\n",
        "val_ds_eval = maybe_trim(val_ds_eval, MAX_VAL_CHUNKS, SEED + 1)\n",
        "print('eval val chunks =', len(val_ds_eval))\n",
        "\n",
        "baseline_val_ppl = compute_ppl(base_model, val_ds_eval, batch_size=EVAL_BATCH_SIZE)\n",
        "print('baseline validation perplexity =', round(baseline_val_ppl, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Optional Training from HF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_model = None\n",
        "trained_tokenizer = None\n",
        "stage_summaries = []\n",
        "post_val_ppl = None\n",
        "\n",
        "if not RUN_TRAINING:\n",
        "    print('RUN_TRAINING=False, skipping training. Notebook can still evaluate HF model in final step.')\n",
        "else:\n",
        "    trained_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
        "    if trained_tokenizer.pad_token is None:\n",
        "        trained_tokenizer.pad_token = trained_tokenizer.eos_token\n",
        "\n",
        "    trained_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=dtype,\n",
        "    )\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    )\n",
        "    trained_model = get_peft_model(trained_model, lora_cfg)\n",
        "    trained_model.to(device)\n",
        "    trained_model.print_trainable_parameters()\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=trained_tokenizer, mlm=False)\n",
        "    epochs_per_stage = TOTAL_EPOCHS / len(SEQUENCE_CURRICULUM)\n",
        "\n",
        "    for stage_idx, block_size in enumerate(SEQUENCE_CURRICULUM, start=1):\n",
        "        print(f'\\n[Stage {stage_idx}] block_size={block_size}')\n",
        "\n",
        "        train_ds = build_chunk_dataset(train_split, trained_tokenizer, block_size)\n",
        "        val_ds = build_chunk_dataset(val_split, trained_tokenizer, block_size)\n",
        "\n",
        "        train_ds = maybe_trim(train_ds, MAX_TRAIN_CHUNKS, SEED + stage_idx)\n",
        "        val_ds = maybe_trim(val_ds, MAX_VAL_CHUNKS, SEED + 100 + stage_idx)\n",
        "\n",
        "        print('train_chunks =', len(train_ds), '| val_chunks =', len(val_ds))\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir=str(OUTPUT_DIR / f'stage_{stage_idx}_bs{block_size}'),\n",
        "            num_train_epochs=epochs_per_stage,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            max_grad_norm=MAX_GRAD_NORM,\n",
        "            eval_strategy='epoch',\n",
        "            save_strategy='no',\n",
        "            logging_steps=10,\n",
        "            report_to='none',\n",
        "            fp16=(device == 'cuda'),\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            seed=SEED + stage_idx,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=trained_model,\n",
        "            args=args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=val_ds,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        train_result = trainer.train()\n",
        "        eval_result = trainer.evaluate(eval_dataset=val_ds)\n",
        "\n",
        "        stage_summaries.append({\n",
        "            'stage': stage_idx,\n",
        "            'block_size': block_size,\n",
        "            'epochs': epochs_per_stage,\n",
        "            'train_chunks': len(train_ds),\n",
        "            'val_chunks': len(val_ds),\n",
        "            'training_loss': float(train_result.training_loss),\n",
        "            'validation_loss': float(eval_result.get('eval_loss', float('nan'))),\n",
        "        })\n",
        "\n",
        "    post_val_ds = build_chunk_dataset(val_split, trained_tokenizer, EVAL_BLOCK_SIZE)\n",
        "    post_val_ds = maybe_trim(post_val_ds, MAX_VAL_CHUNKS, SEED + 1)\n",
        "    post_val_ppl = compute_ppl(trained_model, post_val_ds, batch_size=EVAL_BATCH_SIZE)\n",
        "    print('post-training validation perplexity =', round(post_val_ppl, 4))\n",
        "\n",
        "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    trained_model.save_pretrained(str(MODEL_DIR))\n",
        "    trained_tokenizer.save_pretrained(str(MODEL_DIR))\n",
        "\n",
        "    metrics = {\n",
        "        'track': 'Track 1',\n",
        "        'dataset_repo': HF_DATASET_REPO,\n",
        "        'base_model_id': BASE_MODEL_ID,\n",
        "        'sequence_curriculum': SEQUENCE_CURRICULUM,\n",
        "        'eval_block_size': EVAL_BLOCK_SIZE,\n",
        "        'epochs': TOTAL_EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'eval_batch_size': EVAL_BATCH_SIZE,\n",
        "        'grad_accum_steps': GRAD_ACCUM_STEPS,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'max_grad_norm': MAX_GRAD_NORM,\n",
        "        'train_files': len(train_split),\n",
        "        'val_files': len(val_split),\n",
        "        'baseline_validation_perplexity': round(baseline_val_ppl, 4),\n",
        "        'post_training_validation_perplexity': round(post_val_ppl, 4),\n",
        "        'perplexity_reduction': round(baseline_val_ppl - post_val_ppl, 4),\n",
        "        'improvement_percent': round(((baseline_val_ppl - post_val_ppl) / baseline_val_ppl) * 100, 4),\n",
        "        'stage_summaries': stage_summaries,\n",
        "    }\n",
        "\n",
        "    METRICS_FILE.write_text(json.dumps(metrics, indent=2))\n",
        "    print('saved metrics ->', METRICS_FILE)\n",
        "    print('saved model   ->', MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Final Perplexity Display from HF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This always works independently of notebook training if HF_FINAL_MODEL_REPO exists.\n",
        "final_tokenizer = AutoTokenizer.from_pretrained(HF_FINAL_MODEL_REPO, trust_remote_code=True, fix_mistral_regex=True)\n",
        "if final_tokenizer.pad_token is None:\n",
        "    final_tokenizer.pad_token = final_tokenizer.eos_token\n",
        "\n",
        "final_model = AutoModelForCausalLM.from_pretrained(\n",
        "    HF_FINAL_MODEL_REPO,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=dtype,\n",
        ")\n",
        "final_model.to(device)\n",
        "\n",
        "final_val_ds = build_chunk_dataset(val_split, final_tokenizer, EVAL_BLOCK_SIZE)\n",
        "final_val_ds = maybe_trim(final_val_ds, MAX_VAL_CHUNKS, SEED + 1)\n",
        "final_val_ppl = compute_ppl(final_model, final_val_ds, batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "print('HF final model repo                  =', HF_FINAL_MODEL_REPO)\n",
        "print('validation chunks used              =', len(final_val_ds))\n",
        "print('final model validation perplexity   =', round(final_val_ppl, 4))\n",
        "\n",
        "if METRICS_FILE.exists():\n",
        "    m = json.loads(METRICS_FILE.read_text())\n",
        "    print('recorded post-training val perplexity =', m.get('post_training_validation_perplexity'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Notes\n",
        "\n",
        "- Set `RUN_TRAINING=True` if you want end-to-end training in this notebook.\n",
        "- If you only need a final perplexity check, keep `RUN_TRAINING=False` and run the last cell.\n",
        "- For adapter-only HF repos, load base model + adapter with `PeftModel.from_pretrained(...)`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
