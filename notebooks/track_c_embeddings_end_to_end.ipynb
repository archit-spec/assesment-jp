{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3f2cd1e",
      "metadata": {},
      "source": "# Track C: Self-Contained Embedding Fine-Tuning (HF Dataset)\\n\n\\n\nThis notebook is fully self-contained. It: \\n\n1. Loads retrieval data directly from a Hugging Face dataset repo (`train.jsonl` / `test.jsonl`)\\n\n2. Converts rows into textâ†’code pairs\\n\n3. Fine-tunes an embedding model with in-batch negatives\\n\n4. Evaluates **Accuracy@1**, **MRR@10**, **nDCG@10**, **Recall@10** before and after training"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b53f2094",
      "metadata": {},
      "outputs": [],
      "source": "# Uncomment if needed:\\n\n# !pip install -q sentence-transformers datasets huggingface_hub numpy torch\\n\nimport json\nimport math\\n\nimport random\\n\nfrom pathlib import Path\\n\nfrom typing import Dict, List\\n\n\\n\nimport numpy as np\\n\nimport torch\\n\nfrom torch.utils.data import DataLoader\\n\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\\n\nfrom huggingface_hub import hf_hub_download\\n"
    },
    {
      "cell_type": "markdown",
      "id": "4b7bf68a",
      "metadata": {},
      "source": "## Config"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ef497a",
      "metadata": {},
      "outputs": [],
      "source": "# HF dataset repo with train.jsonl and test.jsonl\\n\nHF_DATASET_REPO = \"archit11/assesment_embeddings_new\"\\n\nHF_TRAIN_FILE = \"train.jsonl\"\\n\nHF_TEST_FILE = \"test.jsonl\"\\n\n\\n\n# Embedding model choices: BAAI/bge-small-en-v1.5, intfloat/e5-small-v2, thenlper/gte-small\\n\nMODEL_NAME = \"BAAI/bge-small-en-v1.5\"\\n\n\\n\nSEED = 42\\n\nEPOCHS = 4\\n\nBATCH_SIZE = 32\\n\nLR = 2e-5\\n\nWARMUP_RATIO = 0.1\\n\nTOP_K = 10\\n\nMAX_TRAIN_PAIRS = 0   # 0 = all\\n\nMAX_TEST_PAIRS = 0    # 0 = all\\n\nOUTPUT_DIR = Path(\"results/track_c_notebook\")\\n\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\\n\n\\n\nrandom.seed(SEED)\\n\nnp.random.seed(SEED)\\n\ntorch.manual_seed(SEED)\\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\nprint(\"Device:\", device)"
    },
    {
      "cell_type": "markdown",
      "id": "94e19e35",
      "metadata": {},
      "source": "## Load Dataset From HF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ff8683",
      "metadata": {},
      "outputs": [],
      "source": "def load_jsonl_from_hf(repo_id: str, filename: str) -> List[dict]:\\n\n    local_path = hf_hub_download(repo_id=repo_id, repo_type=\"dataset\", filename=filename)\\n\n    rows = []\\n\n    with open(local_path, \"r\", encoding=\"utf-8\") as f:\\n\n        for line in f:\\n\n            line = line.strip()\\n\n            if not line:\\n\n                continue\\n\n            rows.append(json.loads(line))\\n\n    return rows\\n\n\\n\ndef to_query_code_pairs(rows: List[dict]) -> List[dict]:\\n\n    # Supports rows like {queries:[...], anchor:...} OR {query:..., code:...}\\n\n    out = []\\n\n    for r in rows:\\n\n        if \"query\" in r and \"code\" in r:\\n\n            q = str(r[\"query\"]).strip()\\n\n            c = str(r[\"code\"]).strip()\\n\n            if q and c:\\n\n                out.append({\"query\": q, \"code\": c, \"meta\": r})\\n\n            continue\\n\n\\n\n        anchor = str(r.get(\"anchor\", \"\")).strip()\\n\n        if not anchor:\\n\n            continue\\n\n        for q in r.get(\"queries\", []):\\n\n            q = str(q).strip()\\n\n            if q:\\n\n                out.append({\"query\": q, \"code\": anchor, \"meta\": r})\\n\n    return out\\n\n\\n\ntrain_raw = load_jsonl_from_hf(HF_DATASET_REPO, HF_TRAIN_FILE)\\n\ntest_raw = load_jsonl_from_hf(HF_DATASET_REPO, HF_TEST_FILE)\\n\ntrain_pairs = to_query_code_pairs(train_raw)\\n\ntest_pairs = to_query_code_pairs(test_raw)\\n\n\\n\nif MAX_TRAIN_PAIRS > 0:\\n\n    train_pairs = train_pairs[:MAX_TRAIN_PAIRS]\\n\nif MAX_TEST_PAIRS > 0:\\n\n    test_pairs = test_pairs[:MAX_TEST_PAIRS]\\n\n\\n\nprint(f\"Train pairs: {len(train_pairs)}\")\\n\nprint(f\"Test pairs:  {len(test_pairs)}\")"
    },
    {
      "cell_type": "markdown",
      "id": "b11ae6ec",
      "metadata": {},
      "source": "## Helpers: Formatting + Metrics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43d990e",
      "metadata": {},
      "outputs": [],
      "source": "def format_query(query: str, model_name: str) -> str:\\n\n    m = model_name.lower()\\n\n    q = query.strip()\\n\n    if \"e5\" in m:\\n\n        return f\"query: {q}\"\\n\n    if \"bge\" in m:\\n\n        return f\"Represent this sentence for searching relevant code: {q}\"\\n\n    return q\\n\n\\n\ndef format_code(code: str, model_name: str) -> str:\\n\n    m = model_name.lower()\\n\n    c = code.strip()\\n\n    if \"e5\" in m:\\n\n        return f\"passage: {c}\"\\n\n    return c\\n\n\\n\ndef compute_metrics(model: SentenceTransformer, pairs: List[dict], model_name: str, k: int = 10) -> Dict[str, float]:\\n\n    # Build unique code corpus\\n\n    code_to_idx = {}\\n\n    codes = []\\n\n    queries = []\\n\n    target_idx = []\\n\n\\n\n    for item in pairs:\\n\n        code = format_code(item[\"code\"], model_name)\\n\n        if code not in code_to_idx:\\n\n            code_to_idx[code] = len(codes)\\n\n            codes.append(code)\\n\n\\n\n    for item in pairs:\\n\n        q = format_query(item[\"query\"], model_name)\\n\n        c = format_code(item[\"code\"], model_name)\\n\n        queries.append(q)\\n\n        target_idx.append(code_to_idx[c])\\n\n\\n\n    q_emb = model.encode(queries, convert_to_numpy=True, show_progress_bar=True)\\n\n    c_emb = model.encode(codes, convert_to_numpy=True, show_progress_bar=True)\\n\n\\n\n    q_emb = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)\\n\n    c_emb = c_emb / np.linalg.norm(c_emb, axis=1, keepdims=True)\\n\n    sim = np.dot(q_emb, c_emb.T)\\n\n\\n\n    mrr, ndcg, recall, acc1 = [], [], [], []\\n\n    for i in range(len(queries)):\\n\n        correct = target_idx[i]\\n\n        topk = np.argsort(sim[i])[::-1][:k]\\n\n\\n\n        # Accuracy@1\\n\n        acc1.append(1.0 if topk[0] == correct else 0.0)\\n\n\\n\n        # Recall@k\\n\n        recall.append(1.0 if correct in topk else 0.0)\\n\n\\n\n        # MRR@k\\n\n        rr = 0.0\\n\n        for rank, idx in enumerate(topk, start=1):\\n\n            if idx == correct:\\n\n                rr = 1.0 / rank\\n\n                break\\n\n        mrr.append(rr)\\n\n\\n\n        # nDCG@k (single relevant item)\\n\n        dcg = 0.0\\n\n        for rank, idx in enumerate(topk, start=1):\\n\n            rel = 1.0 if idx == correct else 0.0\\n\n            dcg += rel / math.log2(rank + 1)\\n\n        idcg = 1.0\\n\n        ndcg.append(dcg / idcg)\\n\n\\n\n    return {\\n\n        \"Accuracy@1\": round(float(np.mean(acc1)), 4),\\n\n        f\"MRR@{k}\": round(float(np.mean(mrr)), 4),\\n\n        f\"nDCG@{k}\": round(float(np.mean(ndcg)), 4),\\n\n        f\"Recall@{k}\": round(float(np.mean(recall)), 4),\\n\n    }"
    },
    {
      "cell_type": "markdown",
      "id": "b7d51e58",
      "metadata": {},
      "source": "## Baseline"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3937c4c5",
      "metadata": {},
      "outputs": [],
      "source": "baseline_model = SentenceTransformer(MODEL_NAME, device=device)\\n\nbaseline_metrics = compute_metrics(baseline_model, test_pairs, MODEL_NAME, k=TOP_K)\\n\nprint(\"Baseline metrics:\")\\n\nprint(baseline_metrics)"
    },
    {
      "cell_type": "markdown",
      "id": "6ab3645b",
      "metadata": {},
      "source": "## Fine-Tune"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8920ad6e",
      "metadata": {},
      "outputs": [],
      "source": "def build_train_examples(pairs: List[dict], model_name: str):\\n\n    ex = []\\n\n    for item in pairs:\\n\n        ex.append(InputExample(texts=[format_query(item['query'], model_name), format_code(item['code'], model_name)]))\\n\n    return ex\\n\n\\n\ntrain_examples = build_train_examples(train_pairs, MODEL_NAME)\\n\ntrain_loader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\\n\n\\n\nft_model = SentenceTransformer(MODEL_NAME, device=device)\\n\ntrain_loss = losses.MultipleNegativesRankingLoss(ft_model)\\n\nwarmup_steps = int(len(train_loader) * EPOCHS * WARMUP_RATIO)\\n\n\\n\nft_model.fit(\\n\n    train_objectives=[(train_loader, train_loss)],\\n\n    epochs=EPOCHS,\\n\n    warmup_steps=warmup_steps,\\n\n    optimizer_params={\"lr\": LR},\\n\n    output_path=str(OUTPUT_DIR / \"model\"),\\n\n    show_progress_bar=True,\\n\n)"
    },
    {
      "cell_type": "markdown",
      "id": "ed80168e",
      "metadata": {},
      "source": "## Post-Training Accuracy Test"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87be29fc",
      "metadata": {},
      "outputs": [],
      "source": "model_path = OUTPUT_DIR / \"model\"\\n\npost_model = SentenceTransformer(str(model_path), device=device)\\n\npost_metrics = compute_metrics(post_model, test_pairs, MODEL_NAME, k=TOP_K)\\n\n\\n\nprint(\"Post-training metrics:\")\\n\nprint(post_metrics)"
    },
    {
      "cell_type": "markdown",
      "id": "5ee4920d",
      "metadata": {},
      "source": "## Compare Baseline vs Fine-Tuned"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45aed001",
      "metadata": {},
      "outputs": [],
      "source": "keys = [\"Accuracy@1\", f\"MRR@{TOP_K}\", f\"nDCG@{TOP_K}\", f\"Recall@{TOP_K}\"]\\n\nreport = {}\\n\nfor k in keys:\\n\n    b = baseline_metrics[k]\\n\n    p = post_metrics[k]\\n\n    d = round(p - b, 4)\\n\n    pct = float('inf') if b == 0 else round((d / b) * 100.0, 2)\\n\n    report[k] = {\"baseline\": b, \"post\": p, \"delta\": d, \"pct\": pct}\\n\n\\n\nfor k, v in report.items():\\n\n    print(f\"{k:<10} baseline={v['baseline']:.4f}  post={v['post']:.4f}  delta={v['delta']:+.4f}  pct={v['pct']}%\")\\n\n\\n\nmetrics_out = {\\n\n    \"model\": MODEL_NAME,\\n\n    \"dataset_repo\": HF_DATASET_REPO,\\n\n    \"train_pairs\": len(train_pairs),\\n\n    \"test_pairs\": len(test_pairs),\\n\n    \"baseline\": baseline_metrics,\\n\n    \"post_training\": post_metrics,\\n\n    \"improvement\": report,\\n\n}\\n\nwith open(OUTPUT_DIR / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:\\n\n    json.dump(metrics_out, f, indent=2)\\n\nprint(\"Saved:\", OUTPUT_DIR / \"metrics.json\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}